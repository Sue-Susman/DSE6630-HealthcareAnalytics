---
title: "Demo 2: Biomedical & Clinical Informatics"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Scott Eugley, Adeline Casali, Sam Harper"
date: "28 May 2024"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: references.bib  
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(50009)


# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse,
               janitor,
               naniar,
               stringr,
               ggplot2, 
               kableExtra,
               RColorBrewer,
               gridExtra,
               ggrepel,
               e1071,
               caret
)
```

# Introduction

You have just been brought on as a data science consultant for a patient advocacy watchdog. They have asked you to source information about hospital readmissions, because they would like to flag specific geographic areas or types of hospitals where hospital readmission is unexpectedly high. They have heard that datasets exist to investigate these types of questions, but they are not really sure where to begin.

You remember this one data science class you took - and the dataset from the **Centers for Medicare & Medicaid Services** managed by [Medicare.gov](https://www.medicare.gov). You start poking around the [Hospitals](https://data.cms.gov/provider-data/topics/hospitals) dataset and decide it is exactly what you need.

The problem is, this dataset is massive, so you are not only tasked with identifying a reasonable question you can ask from the dataset but managing the cleaning, pre-processing, and exploratory data analysis on the merged datasets you choose to explore.

## Question

For **Project 2**, you are going to choose your own question! But for this demo analysis, we are focusing on **pneumonia-related hospital readmissions**.

### Why pneumonia-related readmissions?

Hospital readmission rates for patients with pneumonia is almost unbelievably high, with about $\frac{1}{5}$ or 20% of patients hospitalized with pneumonia re-admitted within 30 days (de Alba and Amin, 2014). Further, pneumonia is both a leading cause of death among the elderly as well as a commonly communicated disease in hospitals (as in, people may come into a hospital for a different condition but be readmitted for pneumonia). Thus, given that pneumonia is the only communicable disease on this list, it is serving as our best proxy for diseases spread person-to-person within hospitals because of lack of availability.

#### Our research question:

Can we predict pneumonia-related hospital readmissions and, more generally, disease transfer within hospitals based on characteristics of hospitals, including their patient-ratings, how much Medicare money they receive, and other metrics that might indicate how well the hospital is doing at diagnostics and disease prevention?

#### Our hypothesis & prediction:

Hospitals are, by their very nature, hot zones for increased spread of disease, including pneumonia. We will be able to predict pneumonia-related readmissions because of intrinsic or extrinsic factors that affect the quality of care, and thus the likelihood of disease treatment or transmission, because these factors impact pneumonia treatment and spread. We specifically predict that factors indicating lower quality-of-care will predict higher than average rates of pneumonia-related hospital readmissions.

## Objective

Our objective for the patient advocacy watchdog is clean, merge, explore, and predict which hospitals tend to have higher-than-average rates of pneumonia-related readmissions. This is to enable the watchdog group to put together a dashboard or otherwise deploy this information to share with patients so they know how their local hospitals are performing, allowing them to make healthier, more informed choices.

# Data & Data Cleaning

Download the [Hospitals](https://data.cms.gov/provider-data/topics/hospitals) dataset as a zip directory, and unzip it. Make sure to adjust the name and/or path of the directory in your directory if it is different from mine. Note that mine is living on my Desktop.

**WARNING!** Do not try to put these data onto GitHub. They are too large to push. If you need help adjusting your path or have questions, please let me know.

### Read in the hospital-level data iteratively, while dynamically naming & storing each file as a dataframe.

You may have to dig through the accompanying data dictionary **HOSPITAL_Data_Dictionary.pdf** to get a better handle on what is in the hospital-level files.

##### **Question 1** (SE): [1 point]

Take a look at the chunk below, which performs **dynamic variable assignment** as it reads in each of the files. Your task is to add a comment to each line of code to describe what that line does. \> Your answer in the code chunk as comments.

```{r, message=FALSE, warning=FALSE}

# Set the directory for the data files
filepath <- "/Users/adelinecasali/Desktop/hospitals_current_data/" 

# List the files in the directory that have "Hospital.csv"
files <- list.files(path = filepath, pattern = "Hospital.csv")

# Iterate through each file in the list
for(f in 1:length(files)) {
  
# Read the CSV, clean column names to upper camel case, and store in "dat"
    dat <- clean_names(read_csv(paste0(filepath, files[f]),
                                show_col_types = FALSE), 
                       case = "upper_camel")
    
# Remove ".Hospital.csv" part of the file names to create variable name
    filename <- gsub(".Hospital\\.csv", "", files[f])
    
# Assign data to a variable with the above created name
    assign(filename, dat)
}
# Create a df of file names without ".Hospital.csv"
files <- gsub(".Hospital\\.csv", "", files) %>% data.frame()

# Set column name of the df to "File Name"
names(files) <- "File Name"
```

Now, let's look at a list of the files that we've read in:

```{r, echo = FALSE}
files %>% 
  kable(
    format = "html",
    caption = "Table 1. List of hospital-level data files.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

##### **Question 2** (SE): [1 point]

Take a look at some of the dataframes, e.g., `HCAHPS` and `FY_2024_Hospital_Readmissions_Reduction_Program`. What do you notice about the format / structure of the data? Does the data have an issue with the dimensionality? **Hint:** Check to see if each row represents a single line of data or not!

```{r}
# Display first 10 rows of HCAHPS
head(HCAHPS,10)

# Display first 10 rows of FY_2024_Hospital_Readmissions_Reduction_Program 
head(FY_2024_Hospital_Readmissions_Reduction_Program,10)

```

> The data appears to be in long format. Each row does represent a single line of data. My initial impression is that there is not an issue with dimensionality here.

### Tidy the data response variables' dataframe: Fiscal Year 2024 Hospital Readmissions Reduction Program.

We are going to focus on the dataset `FY_2024_Hospital_Readmissions_Reduction_Program` to work on best practices to [tidy our data](https://vita.had.co.nz/papers/tidy-data.pdf). Although I do not personally choose to follow everything that Hadley Wickham and the **Tidy Data** Movement has espoused, I do think a lot of the best practices are useful principles especially when we are just getting use to working with data. Further, we can leverage a lot of the functions to make our lives MUCH easier! And who doesn't like easier?!

##### **Question 3** (SE): [1 point]

Take a deeper look at `FY_2024_Hospital_Readmissions_Reduction_Program` at some of the numeric variables in the dataset, e.g., `NumberOfReadmissions`?

```{r}
# Filter dataset to include numeric columns only
num_vars <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)

# Display first 20 rows of NumberOfReadmissions column
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions, 20)
```

> It appears as though the numeric column, "Footnote" has 12,077 missing values. Interestingly, when filtering for numeric variables, "NumberOfReadmissions" does not appear. Even though when you look at the first 20 rows of that column, you can see missing values. This must mean that the column is not characterizes as numeric.

**Hint:** Columns 5 & 7:10 have problems with the `NA` class.

##### **Question 4** (SE): [1 point]

Investigate the `replace_with_na_all()` function that is part of the `naniar` package. Use this function to fix the `NA` class in the aberrant columns in `FY_2024_Hospital_Readmissions_Reduction_Program`.

```{r}
# Use the function "replace_with_na_all()" to replace aberrant values with NA
FY_2024_Hospital_Readmissions_Reduction_Program <- replace_with_na_all(FY_2024_Hospital_Readmissions_Reduction_Program, condition = ~ .x == "N/A")
```

You will notice that this aberrant class of missing value **caused** the numeric columns to be coerced into character columns when they were imported by `read_csv()`. Although we could try using `as.numeric()` by itself, which would assign `NA` to any non-numeric entry in `FY_2024_Hospital_Readmissions_Reduction_Program`, the problem is that we could lose important information that way. Thus, as annoying as this is, we should fix it a little more specifically...

##### **Question 5** (SE): [1 point]

Look more closely at the `NumberOfReadmissions` column in the `FY_2024_Hospital_Readmissions_Reduction_Program`. What other issue do you see with this specific column that is coercing it to the character type, and why does it exist?

**Hint:** Make sure to use the accompanying data dictionary **HOSPITAL_Data_Dictionary.pdf** to also examine **WHY** this problem exists!

> The other issue coercing this variable into character type, is the occurrences of "Too Few to Report". This exists because the data is mostly concerned with looking at excess rates of readmission. Reporting individual counts for low numbers of readmission also poses identification risks to specific patients.

##### **Question 6** (SE): [1 point]

Investigate the `gsub()` function that is part of base `R` or any other function of your choosing. Use the function you choose to **replace** the problematic text in the `NumberOfReadmissions` column in `FY_2024_Hospital_Readmissions_Reduction_Program` with a **5**.

```{r}
# Replace "Too Few to Report" values with "5" in using gsub
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- gsub("Too Few to Report", "5", FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Check first 10 rows to confirm that it worked
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions, 10)
```

##### **Question 7** (SE): [1 point]

Speculate as to why I asked you to replace the text with a **5** rather than a **0** or an `NA`. Can you think of any other logical choices for the substitution? Do you agree with my choice? (You're free to disagree, but please defend your decision either way!)

> Replacing the text with a 5 makes sense for a few reasons. A value of 5 maintains the intent that the number of readmissions is too low to report, as the minimum reportable value of readmission is 11. This also avoids using 0, which would be misleading, because being too low to report isn't the same as zero readmissions. NA indicates missing data, while a value of 5 specifically indicates a low reported value. Using 5 is a great way to keep the column numeric and indicate a value is too low to report while not reporting it as 0 or as missing (NA).

##### **Question 8** (SE): [1 point]

Now, imagine an alternative scenario where instead of replacing it with a **5**, I instead asked you to replace it with a randomly sampled integer from 1 to 10. What would this do? Would `gsub()` work here or would I need to execute it a different way, e.g., with an `lapply()` function or a `for()` loop?

> This would introduce variability into the replaced values, which could possibly represent a more realistic distribution of readmission counts, especially at the low end. gsub would not work here. Either "lapply()" function or "for()" loop should be used.

##### **Question 9** (SE): [3 points]

You probably anticipated this! We just decided that, rather than a **5** we want a randomly sampled integer from 1 to 10.

**Hint 1:** You might find it easier to read the data in from fresh and, after commenting out your `gsub` from Question 5, pattern match on the problematic text.

**Hint 2:** This is NOT the only way to do this, but I solved this using `gregexpr()`, `regmatches()`, and `lapply()`.

```{r}
# NumberOfReadmissions had to be converted to numeric before applying integers
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- as.numeric(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Find all values of "5" in NumberOfReadmissions
fives <- which(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions == 5)

# Replace values of "5" with random integers from 1 - 10
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions[fives] <- sample(1:10, length(fives), replace = TRUE)

# Check the first 20 rows to see if this was applied correctly
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions,20)

```

##### **Question 10** (AC): [1 point]

It is finally time to fix those aberrant character columns by making them numeric. However, just using `as.numeric()` would require us to do it over and over on each of the columns in multiple lines of code. Instead, investigate the function `mutate_at()` which will allow you to pass it a list of columns you want to convert with the function you want to use.

```{r}
# Selecting the columns to convert
columns_to_convert <- c("NumberOfDischarges", "ExcessReadmissionRatio", "PredictedReadmissionRate", "ExpectedReadmissionRate", "NumberOfReadmissions")

# Use mutate_at to convert the specified columns to numeric
FY_2024_Hospital_Readmissions_Reduction_Program <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate_at(vars(one_of(columns_to_convert)), as.numeric)

# Print the structure of the dataframe to check the changes
str(FY_2024_Hospital_Readmissions_Reduction_Program)
```

#### Let's look more closely at Measure Names.

Notice, that what I have done here first is used the `gsub()` function to `mutate()` the `MeasureName` column to remove the text that flanked the medical conditions.

```{r}
FY_2024_Hospital_Readmissions_Reduction_Program <-  FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate(MeasureName = gsub("READM-30-", "", MeasureName)) %>% 
  mutate(MeasureName = gsub("-HRRP", "", MeasureName)) 
```

Let's look at what medical conditions the acronyms stand for:

```{r}
dict <- tribble(
  ~Acronym, ~Definition,
  "HIP-KNEE", "Total Hip/Knee Arthroplasty",
  "HF", "Heart Failure",
  "COPD", "Chronic Obstructive Pulmonary Disease",
  "AMI", "Acute Myocardial Infarction",
  "CABG", "Coronary Artery Bypass Graft",
  "PN", "Pneumonia"
)
dict %>% 
  kable(
    format = "html",
    caption = "Table 2. Acronyms of medical conditions for which hospital readmissions are tracked.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

It's important to understand that these medical conditions are the **ones that Medicaid/Medicare tracks for hospital readmissions**. These are not the only conditions in which a patient might be readmitted, but these are the ones that the agency uses to keep track of hospital performance.

##### **Question 11** (AC): [1 point]

Investigate the function `pivot_wider()` or `spread()` from the `tidyr` package, which comes bundled with `tidyverse`. Try pivoting the `FY_2024_Hospital_Readmissions_Reduction_Program` dataset wider. Make sure you correctly identify the `names_from` and `values_from` column(s). \*\*Make sure to save it as a separate dataframe called `wideDF`. Use the `dim()` function to prove to me that you successfully pivoted wider.

```{r}
wideDF <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  pivot_wider(
    names_from = MeasureName, 
    values_from = c(NumberOfDischarges, ExcessReadmissionRatio, PredictedReadmissionRate, ExpectedReadmissionRate, NumberOfReadmissions), 
    id_cols = c(FacilityName, FacilityId, State, StartDate, EndDate)
  )

# Check the new dataframe
dim(wideDF)
```

**Note:** If at this point you've had any issues with any of the previous problems 2-10, you can load the cleaned readmissions data, `readmissionsClean.Rdata`.

```{r}
load(file = "readmissionsClean.Rdata")
```

#### Filtering for specific conditions.

Ideally, though, before we'd pivot wider we would decide if we were going to filter for any specific hospital readmission conditions. For your **Project 2**, you will be choosing which medical condition(s) you want to focus on for predicting hospital readmission. However, for the remainder of this demo, we're going to focus on just **pneumonia**.

**Ideas you might consider for your project are:**

-   surgical interventions (`HIP-KNEE`, `CABG`)
-   heart-related conditions (`HF`, `AMI`, & `CABG`)
-   all conditions (warning: this might be too unwieldy!)
-   any other condition(s) that you motivate with a statement or two in support

Therefore, let's filter for just pneumonia-related readmissions which will actually eliminate the need to pivot wider in just this case (because we only chose a single condition).

```{r}
readmissionsClean <- 
  readmissionsClean %>% 
  filter(MeasureName == "PN")
```

### Dynamic creation of merged dataframe objects: supporting dataframes.

We will often be asked to deal with large, unwieldy datasets and sometimes we need to be able to handle them dynamically. **Dynamic variable assignment**, as I demonstrated when we read in the files, is going to come in handy - but it can sometimes be tricky to accomplish. The whole point, though, is to create code that can handle ANYTHING you throw at it.

In a similar vein, we can also perform **dynamic dataframe creation** when we need to modify and stitch together more than one pre-existing dataframe - ESPECIALLY when we might decide down the line to alter our choices by adding or removing other dataframes.

#### Writing your own function to create objects dynamically.

We are going to take all the cleaning steps we did above that was not specific to the `FY_2024_Hospital_Readmissions_Reduction_Program` dataframe and tidy, pivot, & filter for specific readmission conditions, and iteratively join as many dataframes as needed. The goal is to make something flexible enough that, if you change a set of inputs, it will create a joined table from ANY set of dataframe inputs. There is more than one way to do this; but leveraging some of the existing `tidyverse` and `tidyr` functions will likely help you make this very flexible. Please note, however, that you do not have to use those packages.

I start you out with something I find helpful when joining together dataframes that sometimes have *redundant* columns BUT not every column is in every dataframe. I start by making a dataframe called `hospitalInfo` that contains the pertinent information on each hospital in the dataset, so you can drop those columns from the other dataframes to prevent inflation or issues when you later join the dataframes together. You may want to join each of the other dataframes to `hospitalInfo`.

Also, as I was writing my own function, I found that the `Payment and Value of Care` table must be separated into two tables to join well, at least with how I wrote my function. So, I also do that for you below. I also clean up `HCAHPS` to make it easier to work for the function I wrote, so it may also be useful for you. Note that what I'm doing in these cases is making sure that `MeasureName` is the title of the column **I will want to join on**.

##### **Question 12** (AC): [5 points]

Write a function that will (1) fix any issues identified previously, e.g., issues with the `NA` class or pivoting, if needed; (2) join any number of a list of dataframes you give it; and (3) filters the data for given condition(s) prior to pivoting.

For full credit, make sure that your function can clean up and join together at least three of the dataframes. **Note**: Make sure to join with `readmissionsClean` at the end!

-   **Hint 1:** Feel free to drop the `startDate` and `endDate` columns, as well as the `footnote` column. I would even suggest dropping `MeasureId`.

-   **Hint 2:** `select(-any_of(c(...)))` can be used to drop any of a list of columns, regardless of whether it exists in every dataframe or not.

-   **Hint 3:** If you choose to drop all those extraneous columns, then the column you will need to merge/join on will always be in the second position, `MeasureName`.

-   **Hint 4:** `full_join()` in `dplyr` is likely what you will need; you will want to execute the join on `FacilityId`.

-   **Hint 5:** Make sure to check for and/or remove duplicate rows.

-   **Hint 6:** If you are really stuck, move ahead and I give you a cleaned, merged data set to work with.

```{r, warning = FALSE, message = FALSE, echo=FALSE}
## First, make sure to separate the Payment and Values tables, giving each facility IDs:
paymentOnly <- Payment_and_Value_of_Care %>% 
  select(FacilityId, PaymentMeasureName, PaymentCategory, Payment) %>% 
  mutate(Payment = gsub("\\$", "", Payment)) %>% # Remove the dollar sign
  mutate(Payment = gsub("\\,", "", Payment)) %>% # Remove the comma 
  rename(MeasureName = PaymentMeasureName)  # Make consistent with other tables

valueOnly <- Payment_and_Value_of_Care %>% 
  select(FacilityId, ValueOfCareDisplayName, ValueOfCareCategory) %>% 
  rename(MeasureName = ValueOfCareDisplayName)  # Make consistent with other tables

# HCAHPS is also a mess. Remove the columns that we will never use in any analysis, make a MeasureName column, and also drop the " - linear mean score" from some rows in the newly minted MeasureName
HCAHPS <- HCAHPS %>% 
  select(-HcahpsMeasureId, -PatientSurveyStarRatingFootnote, 
         -HcahpsAnswerPercentFootnote, -NumberOfCompletedSurveysFootnote,
         -SurveyResponseRatePercentFootnote, -HcahpsAnswerDescription,
         -PatientSurveyStarRating) %>% 
  rename(MeasureName = HcahpsQuestion) %>% 
  mutate(MeasureName = gsub(" - linear mean score", "", MeasureName))

## Choosing to pull the hospital information off of the Payment and Value table because it is complete there:
hospitalInfo <- Payment_and_Value_of_Care %>% 
                select(FacilityId, FacilityName, Address, CityTown, 
                         State, ZipCode, CountyParish, TelephoneNumber)
```

Okay, take a stab at it!

```{r}
merge_and_clean <- function(dataframes, condition = NULL) {
  # Initialize a list to store cleaned and filtered dataframes
  cleaned_dataframes <- list()
  
  # Loop through each dataframe
  for (i in 1:length(dataframes)) {
    # Clean the dataframe
    print(paste("Processing dataframe", i, "out of", length(dataframes)))
    cleaned_df <- dataframes[[i]] %>%
      # Remove unnecessary columns
      select(-any_of(c("StartDate", "EndDate", "Footnote", "MeasureId"))) %>%
      # Rename columns to upper camel case
      clean_names(case = "upper_camel") %>%
      # Convert aberrant values to NA
      replace_with_na_all(condition = ~ .x == "N/A") %>%
      # Replace "Too Few to Report" with 5 in NumberOfReadmissions column
      mutate(NumberOfReadmissions = ifelse(NumberOfReadmissions == "Too Few to Report", "5", NumberOfReadmissions)) %>%
      # Convert to numeric for existing columns
      mutate(across(where(is.numeric), as.numeric))
    
    # Add cleaned dataframe to the list
    cleaned_dataframes[[i]] <- cleaned_df
    print(paste("Processed dataframe", i))
  }
  
  # Join all cleaned dataframes on FacilityId
  merged_df <- cleaned_dataframes %>% 
    reduce(full_join, by = "FacilityId")
  
  # Filter the merged dataframe based on condition
  if (!is.null(condition) && any(merged_df$MeasureName %in% condition)) {
    merged_df <- merged_df %>%
      filter(MeasureName %in% condition)
  }
  
  # Remove duplicate rows
  merged_df <- distinct(merged_df)
  
  # Pivot wider
  merged_df <- merged_df %>%
    pivot_wider(
      names_from = MeasureName, 
      values_from = c(NumberOfDischarges, ExcessReadmissionRatio, PredictedReadmissionRate, ExpectedReadmissionRate, NumberOfReadmissions)
    )
  
  # Join with readmissionsClean
  merged_df <- merged_df %>%
    left_join(readmissionsClean, by = "FacilityId")
  
  # Return the merged and cleaned dataframe
  return(merged_df)
}

# Test the function by merging and cleaning FY_2024_Hospital_Readmissions_Reduction_Program, Healthcare_Associated_Infections, Unplanned_Hospital_Visits, and readmissionsClean
# I commented it out as I couldn't get it to run past the first dataset. It is successful for the first one and then runs infinitely for the second. 
#merged_data <- merge_and_clean(list(FY_2024_Hospital_Readmissions_Reduction_Program, Healthcare_Associated_Infections, Unplanned_Hospital_Visits), condition = "PN")
```

### The full, merged, tidied dataset for pneumonia.

```{r}
load("pneumoniaFull.Rdata")
```

I ultimately chose to merge **eight** of the datasets together, although I selectively filtered to focus on my question: **Can we predict pneumonia-related hospital readmissions based on factors that might indicate how well the hospital is doing at diagnostics and disease prevention?**

```{r, echo = FALSE, collapse=TRUE}
dict <- tribble(
~Dataset, ~`Information Used`, ~Rationale,
"Healthcare_Associated_Infections", "MRSA Bacteremia", "Hospital-transmitted disease rate",

"paymentOnly", "Mean Medicare payment per patient received by hospital", "",

"Outpatient_Imaging_Efficiency", "Abdomen CT Use of Contrast Material", "Proxy for imaging ability to diagnose pneumonia appropriately",

"Complications_and_Deaths", "Death rate for pneumonia patients", "",

"Complications_and_Deaths", "Perioperative pulmonary embolism or deep vein thrombosis rate", "Diagnostic ability of hospital",

"Complications_and_Deaths", "CMS Medicare PSI 90: Patient safety and adverse events composite", "General hospital safety",

"Complications_and_Deaths", "Postoperative respiratory failure rate", "General ability for respiratory diagnostics",

"Medicare_Hospital_Spending_Per_Patient", "Score", "Score rather than $ amount",

"Timely_and_Effective_Care", "Healthcare workers given influenza vaccination", "Likelihood of influenza transmission",

"Timely_and_Effective_Care", "% healthcare personnel completed COVID-19 primary vaccination series", "Likelihood of COVID-19 transmission",

"Timely_and_Effective_Care", "Average (median) minutes patients spent in the ED before leaving (lower is better)", "Proxy for overall effective diagnosis & treatment",

"Timely_and_Effective_Care", "Left before being seen in ED", "Proxy for overall effective diagnosis & treatment",

"Timely_and_Effective_Care", "Venous Thromboembolism Prophylaxis", "Diagnostic ability for another critical and silent disease",

"Timely_and_Effective_Care", "ICU Thromboembolism Prophylaxis", "Diagnostic ability for another critical and silent disease",

"Timely_and_Effective_Care", "Emergency department volume", "Proxy for how overwhelmed the hospital is",
    
"Unplanned_Hospital_Visits", "Hospital return days for pneumonia patients", "",

"HCAHPS", "Nurse communication", "Overall rating as linear score",

"HCAHPS", "Doctor communication", "Overall rating as linear score",

"HCAHPS", "Staff responsiveness", "Overall rating as linear score",

"HCAHPS", "Communication about medicines", "Overall rating as linear score",

"HCAHPS", "Discharge information", "Overall rating as linear score",

"HCAHPS", "Care transition", "Overall rating as linear score",

"HCAHPS", "Cleanliness", "Overall rating as linear score",

"HCAHPS", "Quietness", "Overall rating as linear score",

"HCAHPS", "Overall hospital rating", "Overall rating as linear score",

"HCAHPS", "Recommend hospital", "Overall rating as linear score",
)

dict %>% 
  kable(
    format = "html",
    caption = "Table 3. List of hospital-level data sets chosen for the pneumonia-related readmissions analysis.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

Whew! Notice that this dataset currently has **100 features** and **4,816 rows**. That's definitely too many features, so let's perform some additional feature selection.

# First Pass: Pre-processing & Feature Selection

Feature selection can take many forms, from what we've already done (choosing features relevant to our question) to culling non-informative columns to using more machine-guided approaches. We're going to leverage all types here.

Additionally, we are going to perform **encoding** to our categorical variables. It will be important to perform encoding **BEFORE** using automated methods for feature selection.

## Culling Features (and more tidying!)

Although we ideally could just include these steps (and perhaps you did!) in our tidying & joining function from Question 12, I wanted to make sure we talk about this explicitly.

##### **Question 13** (AC): [1 point]

Looking through the feature names, you probably notice names like `LowerEstimate_Death rate for pneumonia patients`. We do not need the higher and lower estimates in this case; just the point estimates. Use the `contains()` function to **drop** any columns that contain `LowerEstimate` or `HigherEstimate`. Let's also go ahead and drop any columns containing `Denominator` and `HcahpsAnswerPercent` as well. How many features did you drop? 
> 25 features were dropped. 

```{r}
# Drop columns containing LowerEstimate, HigherEstimate, Denominator, and HcahpsAnswerPercent
pneumoniaFull <- pneumoniaFull %>%
  select(-contains("LowerEstimate"), -contains("HigherEstimate"),
         -contains("Denominator"), -contains("HcahpsAnswerPercent"))

# Count the number of features dropped
features_dropped = 100 - ncol(pneumoniaFull)
features_dropped
```

**Note:** Please drop `FacilityName`, `TelephoneNumber`, `Address`, & `CityTown` at this stage as well, if they're still in your full dataset.

```{r}
pneumoniaFull <- pneumoniaFull %>%
  select(-FacilityName, -TelephoneNumber, -Address, -CityTown)
ncol(pneumoniaFull)
```

## Encoding

### Mystery encoding! (Well, it won't be a mystery for long.)

##### **Question 14** (AC): [1 point]

We need to perform encoding on all of the categorical columns, ultimately. However, we will focus first on the columns that begin with \``ComparedToNational_`, e.g., `ComparedToNational_Death rate for pneumonia patients`. What kind of encoding should we perform on these columns? Why? **What other column do we also need to perform this kind of encoding on?**

> For the columns beginning with "ComparedToNational_" we should perform ordinal encoding as there is a natural order (ie better, worse, or no different than the national rate). We should also perform this kind of encoding on the column beginning in "PaymentCategory_". 

##### **Question 15**: [3 points]

You probably expected this - attempt to encode those columns the method you selected. It's okay if you can only think of a way to brute-force this for now; I will show you code to help you do it faster after the assignment. :)

-   **Hint 1:** You could use the `contains()` or `startsWith()` functions to help you quickly grab those columns so you can make the changes you want to make.

-   **Hint 2:** You may want to explore the `grepl()` function or regular expressions, as they can allow us to match in a "fuzzy" way.

-   **Hint 3:** You may want to create a temporary dataframe that you execute the changes on and then replace the original columns from there. That way, you don't have to keep reloading the original data if you make mistakes!

-   **Hint 3:** For full credit, don't forget to double check that you managed to preserve all the data! The `table()` function is sufficient here.

```{r}
# Your code here.
```

**Note**: If you struggled with Question 15, load the data below and keep going:

```{r}
load("pneumoniaFullEncoded.Rdata")
```

### Frequency encoding state and county.

We also decide that we want a way to try to preserve and analyze the geographic information without causing terrible overfitting. So, we will try to apply **frequency encoding** to those two categorical columns.

##### **Question 16**: [1 points]

Go through the **frequency encoding** code chunk below and comment each line. What does it do? Make sure to comment why you think the choice was made, if appropriate.

```{r}
cols2encode <- c("State", 
               "CountyParish")

temp <- pneumoniaFullEncoded[, names(pneumoniaFullEncoded) %in% cols2encode]

add_freq <- function(data, column_name) {
  frequency_map <- table(data[[column_name]], useNA = "always")
  data[[column_name]] <- frequency_map[match(data[[column_name]],
                                             names(frequency_map))]
  return(data)
}

for (col in names(temp)) {
  temp <- add_freq(temp, col)
}

for (c in 1:length(cols2encode)) {
  pneumoniaFullEncoded[, cols2encode[c]] <- temp[, cols2encode[c]]
}
```

##### **Question 17**: [1 point]

Use your finally & freshly encoded version of `pneumoniaFullEncoded` to answer this vital question for `State` and `CountyParish` features: are we justified using these features? Why or why not? You will need to write code to answer this question.

**Hint 1:** Use your original `pneumoniaFull` dataset to compare the features in `pneumoniaFullEncoded`.

**Hint 2:** **Heavily** duplicated frequency values can make it ill-advised to proceed with a frequency encoded feature. Low to moderate duplication can be kept if there is a strong justification for doing so.

**Hint 3:** If you decide it is ill-advised for either or both columns, make sure to remove those features from the dataset!

```{r}
<<<<<<< HEAD
# Your code here.
=======
load("~/Documents/GitHub/Merrimack_DSE6630/II. Biomedical & Clinical Informatics/Demo/pneumoniaFull.Rdata")
```


```{r}
length(unique(pneumoniaFull$State))
length(unique(pneumoniaFullEncoded$State))
```
>Above we can see that there is some data loss in the encoded version. This happens when the frequency of two or more unique values in the original data set are the same and so are encoded with the same frequency. The number of unique values for state was 57 and was reduced to 49 after encoding. 


```{r}
length(unique(pneumoniaFull$CountyParish))
length(unique(pneumoniaFullEncoded$CountyParish))
```
>Here there is an even greater loss.

>Because there is so much data lost with this encoding, I don't think it is justified to continue to use these features. 

```{r}
pneumoniaFullEncoded <- subset(pneumoniaFullEncoded, select = -c(State, CountyParish))
>>>>>>> fbe92a69a760c60916a8df7a5bba60f4e73bce92
```

> Your answer here.

### Finishing touches

#### 1. Ensure that every feature is numeric.

```{r, warning=FALSE, message=FALSE}
load("pneumoniaAnalyze.Rdata")

pneumoniaAnalyze <- pneumoniaAnalyze %>% 
  mutate(across(where(is.character), as.numeric))
```

#### 2. Collapse the `NumberOfCompletedSurveys_`... and `SurveyResponseRatePercent_`... features into a single one, as they are identical / redundant:

```{r}
pneumoniaAnalyze <- pneumoniaAnalyze %>% 
  ## arbitrarily chose as the rep as they are identical
  mutate(NumberSurveysCompleted = NumberOfCompletedSurveys_Cleanliness,              
         SurveyResponseRate = SurveyResponseRatePercent_Cleanliness/100) %>%       ## Turned into an actual rate 
  select(-contains(c("NumberOfCompletedSurveys_", "SurveyResponseRatePercent_")))   ## drop the others
```

Excellent! Now we are down to just 51 features...

#### 3. Identify the target variable.

**Our very, very last steps!** Your task is to identify the appropriate target variable from the dataset. I have purposefully not gone into great detail about what it would be because choosing the best target is sometimes tricky.

##### **Question 18**: [2 points]

Use `pneumoniaAnalyze` to calculate an `observed_readmission_rate` as the number of readmissions divided by the number of discharges, multiplied by 100, then drop the two columns used to make this new one. Now, using the data and the data dictionary, try to understand the relationship between `observed_readmission_rate` that we just created, the `PredictedReadmissionRate`, `ExpectedReadmissionRate`, and the `ExcessReadmissionRatio`. What is the relationship? What is the appropriate target to choose and why?

<<<<<<< HEAD
**Hint:** Think about (or perhaps try to investigate)why the **predicted** rate exists. What would be the (dis)advantage to using the predicted vs. the newly created `observed_readmission_rate`?

> Your answer here.
=======
>Predicted readmission is based on the hospital's case load and performance, while the expected readmission rate is based on the national expectations for a case load. 

>Predicted rate is already a form of prediction and not an actual measure of relationships or discharges and readmission at a specific hospital, but rather a reflection of the historical rates and country-wide rates.  The target variable that makes the most sense is the one build by relevant specific data, i.e. our observed_readmission_rate variable. 
>>>>>>> fbe92a69a760c60916a8df7a5bba60f4e73bce92

# Exploratory Data Analysis

For your final question, you will kick start our EDA by focusing on the target variable that you identified in Question 18. **You will not be penalized for choosing the wrong target as long as you made an attempt to choose and defend a rational choice.**

##### **Question 19**: [3 points]

Explore the target variable you identified in Question 18 with at least one other variable. Try to push yourself to try a new style of plot; for example, have you ever made a `density` or `violin` plot? These can be excellent choices when exploring the distribution of a target variable against another variable. Note that, if you choose to compare the target against one of the encoded categorical variables, you will need to properly re-label the categories for your plot.

Full points are awarded for professional plots with axis labels, labeled legends (if appropriate), and creative use of multivariable information. **One bonus point will be awarded if you make a faceted plot or otherwise include information from at least 3 variables.**

Need inspiration that comes with code?!? Check out the [R Graph Gallery](https://r-graph-gallery.com)!

```{r}
# Your code here.
```

Lastly, make sure to **interpret** your graphic.

> Your answer here.

<<<<<<< HEAD
# References
=======
```{r}
pneumoniaAnalyze %>%
  ggplot( aes(x=observed_readmission_rate)) +
    geom_density(fill="#8376a2", color="#e9ecef", alpha=0.8) +
    ggtitle("Observed Readmission Rate") +
    ylab("Density") +
    xlab("Readmission Rate")
```
>The distribution of the the observed readmission rate is roughly normal, but not quite. Most of the rates are centered around 16. The graph seems slightly right skewed as well. 


>Another plot to try some I hadn't used before.
```{r}
#install.packages("beeswarm")
library(beeswarm)
```

```{r}
beeswarm(pneumoniaAnalyze$observed_readmission_rate)
```
>The beeswarm graph doesn't show as much since we have a lot of data but it does show again that the rates are heavily focused between 13 and 22. 


# References
>>>>>>> fbe92a69a760c60916a8df7a5bba60f4e73bce92
