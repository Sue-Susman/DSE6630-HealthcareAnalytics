---
title: "Demo 1: Bioinformatics of Gene Expression"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Katherine S. Geist, PhD"
date: "10 April 2024"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: references.bib  
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(50009)


# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               pheatmap,
               RColorBrewer,
               vsn,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest
)
```

# Introduction

You have just been hired by a university to work on a team with other biological data scientists working on responses of organisms to climate change. Native pollinators are of special importance because of agriculture and food security; but some pollinators are likely to experience thermal intolerance, especially to changes in temperatures that alter their core physiology because of rapid temperature changes.

Exposing bees and other insects to extreme cold puts them into what is often referred to as a **chill coma**: their body functions slow to the point that they go into torpor, which means they cannot move. Their physiology slows to the point that they are unable to move their flight muscles, for example. Here, we have differential gene expression data from the North American bumblebee, *Bombus impatiens*, which is both a native pollinator found throughout most of the U.S and Canada. It is also an important agricultural species because of its pollination services! Chill coma is essential to the survival of *B. impatiens* maiden queens as they overwinter; they need to emerge from their burrows in the ground in the spring to start their nests, lay eggs, rear larvae, and ultimately pollinate our food plants. Thus, understanding the genes underlying the species' physiological responses to shifts in cold temperature, especially in light of predicted temperature shifts in North America, could be particularly important.

You were asked to work with other climate data scientists to re-analyze these chill coma experiment expression data using machine learning methods. Your goal is to perform a **benchmarking study** to see how your results compare to conventional methods. You will start with conventional methods and then apply machine learning methods after that to draw your comparisons.

**These data have never been explored this way before**. You can find the original paper [here](%22https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view%22) by Verble et al. (2023). Not only are you contributing to what we understand about best practices for gene expression analysis but you are doing **novel research** too! The original results by Verble et al. (2023) were very compelling, but these data are a perfect opportunity to dp some benchmarking! You will see why...

## Study Overview

According to the study, *Bombus impatiens* colonies were reared indoors for 72 hours and then treated individuals put into chill coma (Verble et al., 2023) by exposing them to $\approx$ 0$^{\circ}$ C (32$^{\circ} F$ for 75 min. Individuals were then killed at the following intervals to isolate their RNA (and thus measure gene expression at those number of minutes post-coma): 0 min, 10 min, 30 min, 120 min, and 720 min. Bumblebees (cold-treated and control from the same colony) were sacrficied by flash freezing them at -80 to await RNA isolation. This is because RNA is very unstable at room temperature, thus instantaneous death is essential to stop gene expression changes that might be happening in the organism. Overall, you have been given information about the bumblebee's natal colony, whether the sample was cold-exposed or control, and the duration of chill coma.

## Sample Metadata

**Metadata** refer to information about sequencing files that describe the samples. This can include information about how the samples were collected, the **phenotypes** of the samples (the traits, physical or behavioral, that are thought to be at least partially controlled by gene expression diversity), and IDs for individuals or, in this case, colonies from which the data come.

```{r, echo=FALSE, fig.height=3}
# Load the metadata files
metadata <- read_csv("./23524047/Complete_sampleinfo.csv",
                     show_col_types = FALSE) %>% 
            as.data.frame() %>% 
            mutate_if(is.character, as.factor)

dict <- tribble(
  ~ Variable, ~ Description,
  "sampleID", "The original ID of the sequencing sample obtained",
  "Colony", "The bumblebee colony from which the indiduals sampled came",
  "Treatment", "Control vs. Chill Exposure",
  "Time", "Duration of the treatment"
)

kable(
    dict,
    format = "html",
    caption = "Table 1. Metadata of the gene expression samples.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

```{r, echo =FALSE}
head(metadata) 
```

##### **Question 1** (AC): Looking at the [original study](%22https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view%22), method of RNA sequencing was performed? This is an unusual method of sequencing; you might need to look it up. Can you explain, approximately, how it works? (Give your best attempt)

> The researchers in the original study utilized 3' Tag-based RNA sequencing (TagSeq). TagSeq works to sequence RNA at the 3' ends of transcripts by first isolating mRNA and fragmenting. Then, a primer is used and the libraries are amplified with PCR for sequencing in a flowcell.

RNA-sequencing data go through MANY pre-processing steps before they get to this point. This includes:

-   Checking the quality of the FASTQ reads that come off the sequencer
-   Removing ligated barcodes, if needed
-   Performing **alignment** to the genome to know which **genes** are which!
-   Counting the number of reads that **mapped** (aligned) to the genome after ignoring any under a particular quality threshold

##### **Question 2** (AC): Looking at the [original study](%22https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view%22) by briefly describe in 3-4 sentences how the above steps were performed. We will have discussed this some in lecture too if you need help.

> First, the quality of FASTQ reads was assessed using FASTQC software. Next, the recommended tagseq_clipper.pl script was used to clean the reads. Then, STAR software was utilized to align the cleaned reads to the B. impatiens genome and generate counts per locus.

## Read Counts

We will be working with the **raw count** data as **CPM (read counts per million)** from across all 74 samples to perform a **differential gene expression** analysis, often referred to as a 'DE' analysis in abbreviation. I will call it a 'DE' analysis going forward and I will refer to **'differentially expressed genes'** as 'DEGs' or just 'DEG'.

##### **Question 3** (AC): Explain CPM for someone who might not know what that means. You may need to do a little extra reading on the subject!

> Counts per million (CPM) mapped reads are normalized by taking the number of raw reads mapped to a gene, scaling by the total number of mapped reads in a sample, and multiplying by a million. It calculates the expression level of each gene and allows for relative values on a comparable scale.

Let's read in the raw counts and get a sense of these data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
counts <- read_csv(file = "./23524047/ECK_CountData_Complete.csv") %>% as.data.frame()
head(counts[,1:6]) 
```

##### **Question 4** (AC): Why is the `geneID` something like, 'LOC100740276'? What does that mean?

> The geneID format above is commonly used in genomic databases to represent genes. LOC stands for locus, which is the physical locatino of a gene on a chromosome. The numerical part of the ID is a unique idenifier for a particular genomic locus.

##### **Question 5** (AC): For column `COOECK6` gene 'LOC100740276' has a value of 6. What does that mean?

> The value of 6 at that location indicates that the specific gene 'LOC100740276' has 6 read counts for the sample 'C00ECK6', representing the expression level of that gene in that sample.

# Conventional Method: Differential Expression Analysis using `DESeq2`

The Bioconductor package `DESeq2` is one of several such packages that exist in R, but has rapidly become one of the 'gold standards' for DE analyses. You can find more about how this package is used by looking at the [vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html).

## Pre-processing the counts data and metadata

In short, `DESeq2` reads in the raw counts as a matrix, which must then be converted to a **design matrix** which includes the sample metadata for each column. Importantly, matrices should **not** contain the IDs in a column, but should be move to the **row names** of the matrix instead.

#### 1. Assign the `geneID` column as the rownames and then remove the `geneID` column from raw counts.

```{r, echo = TRUE}
rownames(counts) <- counts$geneID
counts <- counts[ ,-1]
```

#### 2. Assign the `sampleID` column as the rownames and then remove the `sampleID` column from metadata.

Just as it was picky about the `geneID` being the **row names** of the counts data, it `DeSeq2` requires the `sampleID` to be the row names of the metadata.

```{r, echo = TRUE}
rownames(metadata) <- metadata$sampleID
metadata <- metadata[ ,-1]
```

#### 3. Configure the metadata with specific column headings.

`DeSEq2` is very picky about the headings of the metadata columns before you make your design matrix. It is critical that the **columns** of the count matrix match the **rows** of the metadata matrix. The order must be identical! If they are not, we need to rearrange them. Note, too, that they must match verbatim.

Let's check that the column names of the `counts` matrix are (1) the same and (2) in the same order as the metadata matrix. Returns `TRUE` for both if these cases are satisfied.

```{r, echo = TRUE, results='asis'}
print(all(colnames(counts) %in% rownames(metadata)))
print(all(colnames(counts) == rownames(metadata)))
```

#### 4. Make sure the levels of the factor(s) of interest in the metadata are using the correct reference level.

There is a fundamental difference in the `character` and `factor` data types, and note that when we read in the original **metadata** dataframe we converted all character type to factor type. This is to allow us to make sure that our levels are in the order we want, with the reference level listed first.

Let's start by taking a look at the levels of `Treatment` - by alphanumeric ordering, `Cold` comes before `Control`, so we need to fix that before we make the **design object** downstream. This is because we want the `Control` condition to be the reference condition!

```{r, results='hide', echo = TRUE}
levels(as.factor(metadata$Treatment))
metadata$Treatment <- factor(metadata$Treatment, levels = c("Control", "Cold"))
```

## Make the `DESeq` Design Object

Once your **raw counts** and **metadata** matrices have been formatted correctly, you are ready to make the **design object**. The design object contains your **model formula** for analysis, which includes both treatment effects and batch effect(s). Notably, the **design object** controls for 'batch differences' -- these are random effects due to differences between individuals sampled, sequencing runs, and/or RNA quantities extracted before sequencing. All of these things could ultimately affect our ability to **statistically detect** differences in gene expression, so controlling for batch effects is essential.

##### **Question 6** (AC): Which variable is the 'batch' variable in this analysis and why? Hint: Look at the design formula in the next code chunk!

> In this analysis, the Colony variable is the batch variable because it represents different batches or experimental groups processed together. The batch variable is used to control for any random effects due to differences between individuals sampled.

The design formula is used to estimate the dispersions and to estimate gene expression changes (measured in log-fold) per the model. For future reference, **you should put the variable of interest at the end of the formula** and make sure the control level is the first level of the factor. E.g., in our case, `Treatment` is placed at the end of the formula as it is the variable of interest. `Time` should also be included after `Treatment`; we expect there to be a difference among the durations of chill coma, potentially.

```{r, message=FALSE, warning=FALSE, results='hide'}
dsgnObject <- DESeqDataSetFromMatrix(countData = counts, 
                                     colData = metadata,
                                     design = ~ Colony + Treatment + Time)
dim(dsgnObject)
```

Notice that, at the same time that we set the **design formula**, we created the actual **design object** upon which the work will actually be run. The `DESeqDataSet` is an object class used by `DeSeq2` to store the read counts, metadata, and design formula.

##### **Question 7** (AC): You may only be used to working with data frames in R, so objects could be a foreign beast. Take a moment to look at the `dsgnObject` we just made. What differences do you notice about the structure vs. a dataframe?

> When looking at the structure of the design object, the first thing that I noticed is that it belongs to a specific type of class specialized for storing RNA-seq count data. Additionally, I noticed that it includes both data as well as specific methods and functions required for RNA-seq analysis. Accessing or viewing the data contained inside of it does not seem to be as straightforward as viewing the data in a traditional dataframe. Overall, its a tool specifically designed for differential analysis.

## Normalization: Variance-Stabilizing Transformation

We currently still have **raw counts** in the design object, which haven't been normalized in any way. We actually have several options for normalization, including regularized-$log_2$ transformation or **variance stabilizing** transformation.

Below is a function that runs a variance stabilizing transform (VST). Note that here I am running a parametric transformation with `blind = FALSE`. We do not want the transformations to be blind to the experimental design (treatment) at this stage; we expect large differential expression in some genes. Thus, we need to control for outliers by setting `blind = FALSE`.

Take a moment to look through the function and try to understand everything it does, including the parameters it takes.

#### Figure 1. Effect of parametric variance-stabilizing transformation on gene expression.

```{r, echo = F, warning=FALSE, message=FALSE}
runVST <- function(dsgnObject, blind, fitType, makePlot = TRUE, writeTable = FALSE, writeRData = FALSE) {
  ## Perform the VST
  
  # Check if the fitType is the regularized log:
  if(fitType == "rlog") {
    vsData <- rlog(dsgnObject, blind = blind)
  }
  ## Otherwise:
  else {
    vsData <- varianceStabilizingTransformation(dsgnObject, 
                                              blind = blind, 
                                              fitType = fitType)
  }
  
  if(makePlot == TRUE) {
    # Plot the effect of the VS transform:
    p1 <- meanSdPlot(assay(dsgnObject), plot = F)
    p1 <- p1$gg + ggtitle("Before Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    p2 <- meanSdPlot(assay(vsData), plot = F)
    p2 <- p2$gg + ggtitle("After Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    grid.arrange(p1, p2, nrow=1)
  }
  
  if(writeTable == TRUE) {
    # Write the data for future use, if needed:
    write.table(assay(vsData),
              file = "vst.txt",
              sep="\t", 
              quote=F, 
              row.names=T)
  }
  if(writeRData == TRUE) {
    save(vsData, file="vst_all_timepoints.Rdata")
  }
  return(vsData)
}

runVST(dsgnObject, blind = FALSE, fitType = "parametric", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)
```

##### **Question 8** (AC): What do you notice about the plot on the left (the `dsgnObject` without any transformation) vs. the plot on the right after the variance stabilizing transform? What do you think the VST is doing?

> The plot on the left has a much larger y axis, indicating a much larger spread of standard deviation. On the other hand, the plot on the right, after variance stabilization, has a much reduced variability with a more consistent pattern of gene expression. VST appears to be normalizing the variance and making the expression levels more uniform and comparable across samples.

##### **Question 9** (AC): This might seem like a lot of work. Why? Think back to other analyses where you have needed to normalize your data. What is the reason we do it, and why does it matter so much that we get it 'right' for our data?

> Normalization is a critical step in data analysis across various fields, from biological research to business intelligence studies. It addresses variability inherent in the data and mitigates differences in experimental conditions, equipment, and sample characteristics. This allows for meaningful comparisons between samples that are easy to interpret and identify patterns or trends.

### Digging Deeper: Estimating Gene-Wise Dispersion

DESeq2 uses a specific measure of dispersion ($\alpha$) related to the mean ($\mu$) and variance of the data: $\sigma^2 = \mu + \alpha * \mu^2$. This means that for genes with moderate to high counts, the square root of dispersion ($\alpha$) equals the Coefficient of Variation ($CV = \sigma^2 / \mu$). Thus, a 0.01 dispersion means there is 10% variation around the mean expected across biological replicates.

#### Figure 2. Gene expression variance by means.

```{r, echo=FALSE}
meanCounts <- rowMeans(assay(dsgnObject))      ## Per locus, what is the average expression
varCounts <- apply(assay(dsgnObject), 1, var)  ## Apply the variance function by margin = 1, which is rows

plot(log(varCounts) ~ log(meanCounts), 
     ylab = "Natural-log Variance in Gene Expression", 
     xlab = "Natural-log Mean Expression", 
     main = "\nLog-Log plot of variance by mean for each gene\n should be approximately linear.\n", 
     pch = 16, 
     cex = 0.75)
abline(lm(log(varCounts+0.0001) ~ log((meanCounts+0.0001))), 
       col = "#a8325e", 
       lty = 2, 
       lwd = 2)
```

The relationship between mean and variance should be linear on the log scale, and in gene expression data we predict that for higher means, we can more accurately predict the variance (i.e., it is more "fanned out" at lower means and a tighter linear relationship at higher means). We expect that for low mean counts, the variance estimates have a much larger spread, such that the dispersion estimates will differ much more between genes with small means. When this pattern holds, a variance-stabilizing transformation will help to resolve this issue. This is because the variance stabilizing transformation (VST) functions provided by `DESeq2` attempt to shrink the gene-wise dispersion.

##### **Question 10** (AC): Does this plot support that a VST is (1) needed, (2) not needed, or (3) likely to be ineffective?

> This plot supports that a VST might be beneficial in stabilizing the variance. The slight upward trajectory at higher mean expression levels indicates increased variance as the points become more fanned out. In this scenario, VST would be beneficial at stabilizing the variance and improving the accuracy of further analysis.

### Other VST Functions

Above, we set the `fitType = "parametric"` when we ran the initial VST. But is this really the best choice? The first rule in data science is **never just use the defaults.**

Other functions you can try: - `fitType = "local"`

-   `fitType = "mean"`

-   `fitType = "rlog"`

##### **Question 11** (SE): Use our custom `runVST()` function, changing out the `fitType` parameter to do tuning. Heuristically, which `fitType` seems to be most appropriate here?

> The "local" or "parametric" fitType appears to be the most appropriate, visually, as it has the lowest spread of standard deviation indicating stable variance. However, the "local" fitType may be more appropriate as it doesn't assume a global relationship between mean and variance. Instead, the "local" fitType assumes that the VST varies across individual features, which may be more appropriate in this type of analysis.  

```{r}
runVST(dsgnObject, blind = FALSE, fitType = "local", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

runVST(dsgnObject, blind = FALSE, fitType = "mean", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

runVST(dsgnObject, blind = FALSE, fitType = "rlog", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

```

## Numbers of DE Genes Based on Fold-Change (Wald Tests)

You're only seeing the tail-end of what is a **very** long process to uncover differential expression. Yet, your work is only just beginning! The first important thing to understand when we are doing **conventional gene expression** analyses is the concept of **fold-change**. We're going to explore this a little bit here before we test for significant differential expression of the genes.

The `DESeq()` function does two things roughly simultaneously. It:

1.  **Performs normalization (median of ratios).** It does this by correcting for variance in read sequencing depth as well as for inter-library dispersion in counts (for each gene).

2.  **Calculates the significance of coefficients with a negative binomial generalized linear model.**

Note that DESeq2 does not actually use the normalized counts but rather uses the raw counts and models the normalization in the negative binomial model. One of the ways that we then make sense of the **magnitude** of significant differential expression is with the **fold-change**.

### What is log-fold change (LFC)?

**Log-fold change** refers to the fold-change on a logarithmic scale to indicate a positive or negative change in expression between two conditions. As a reminder, here we will be comparing the **cold-exposed** treatment to the **control** treatment. Thus, a positive fold change value correlated with increased expression in cold-exposed vs. control whereas a negative fold change indicates a decrease in expression in the cold-exposed treatment relative to control.

However, this value is typically reported in $log_2$, and our human brains do not do on-the-fly conversions of logarithms very well. Let's make ourselves a little cheatsheet to help with this. For example, a $log_2$-fold change of 2 for any given gene in our study would mean that the expression of that gene is increased in **cold-exposed** bees relative to **control** bees, on average, by a multiplicative factor of $2^2 \approx 4$!

#### Figure 3. Relationship between log-fold and linear change.

```{r, echo =FALSE}
lfc <- c(1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)
linearChange <- round(2^lfc, 3)
folds <- cbind.data.frame(lfc, linearChange)

folds %>% 
  ggplot(aes(y = linearChange, x = lfc)) +
  geom_col(fill = "cadetblue", color = "black") +
  labs(x = expression(log[2]-fold),
       y = "Equivalent Linear Change") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  scale_x_continuous(breaks = seq(0, 5, 0.5))

kable(
    folds,
    format = "html",
    col.names = c("log2-fold Change", "Linear Equivalent"),
    caption = "Table 2. Cheatsheet of log-fold change with linear scale equivalents.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

### Wald Tests for significant differential expression between conditions

We have technically performed the contrasts, but we still **do not yet know which genes are differentially expressed!** We will want a standardized way to retrieve DE genes for both the treatments, so we will do hypothesis tests using set significance thresholds. This is a more conservative approach, so you will receive fewer genes than with a post-hoc filtering method, as some papers choose to do. For benchmarking, we will use a conservative method and we can look at the log-fold change to see what thresholds it would correspond to, had we used that method.

This performs a **Wald test** of the specified contrast with a set significance threshold and log-fold change (LFC) threshold. Here, we are using a threshold of $p = 0.05$ since we are applying a Benjamini-Hochberg adjustment to the p-value. We are also using a series of LFC values rather than a single one; otherwise, **choosing one would be rather arbitrary** even if it is often done in the scientific literature! This will allow us to see whether we get very different numbers of differentially expressed genes between the treatment conditions for each of the possible log-fold change cutoffs.

#### 1. Do the normalization you selected as most appropriate in Question 11.

Here I am reading it back in so as not to give away the correct answer! Otherwise, you can just use the `runVST()` function!

```{r, include = TRUE}
runVST(dsgnObject, blind = FALSE, fitType = "local", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

load("vst_all_timepoints.Rdata")
```

#### 2. Estimate the size factors.

A **size factor** is effectively a count of how many reads there are in each library, sample, or batch (it can depend on the design). This is to account for those batch-level effects we discussed earlier! In other words, it's kind of like estimating the degree of proportionality or **weights**. But estimating the size factors is more than merely dividing the counts by the total number of reads in each batch. Here, `DESeq` is scaling the read counts using the third quantile of the distribution of read counts for each sample. It then uses that as a scaling factor to ensure that the third quantile is the **same across all samples**.

```{r, echo=TRUE}
dsgnObject <- estimateSizeFactors(dsgnObject)        ## Yes, we are overwriting our object after scaling for convenience.
# dsgnObject@colData$sizeFactor                      ## If we wanted to see the resulting size factors employed for each sample
```

Let's examine how well the scaling/normalization has worked across all of the samples:

#### Figure 4. Effect of scaling / normalization across all 74 samples.

```{r, fig.width=8, fig.height=8, echo = FALSE}
temp <- metadata %>% 
  mutate(sampleID = rownames(metadata))

normalized_counts_long <- assay(vsData) %>% 
  data.frame() %>%
  mutate(Locus = rownames(assay(vsData))) %>% 
  pivot_longer(cols = -75, values_to = "Variance-Stabilized Expression", names_to = "sampleID")  %>% 
  full_join(temp, by = "sampleID") 

ggplot(normalized_counts_long, aes(x = sampleID, y = `Variance-Stabilized Expression`, color = Treatment)) +
  geom_boxplot() + 
  theme_classic() + 
  coord_flip() + 
  geom_hline(aes(yintercept = median(assay(vsData))), col="black", show.legend = T) +
  scale_color_manual(values=c("cadetblue", "#a8325e"))
```

#### 3. Let's call significantly DE genes!

```{r, echo = TRUE, message=FALSE, warning=FALSE}
alpha <- 0.05                                                         ## Setting this for a False Discovery Rate of 5%
dispObject <- estimateDispersions(dsgnObject)                         ## Estimate the dispersions
waldObject <- nbinomWaldTest(dispObject)                              ## Use that to perform the negative binomial Wald tests
resultsDESeq <- results(waldObject,
                        alpha = alpha, 
                        pAdjustMethod = "BH")                         ## Uses Benjamini-Hochberg / FDR adjusted p-values
summary(resultsDESeq)
#save(resultsDESeq, file = "DESeqResults.Rdata")                       ## Save the results
```

##### **Question 12** (SE): You know what a p-value is but you may not be familiar with an **adjusted p-value**. What is it? Specifically, what's a **False Discovery Rate**?

> When testing multiple variables, such as multiple genes in a genomics study, there is a high risk of false positives, due to chance. Adjusted p-values are used to mitigate this issue. A common method of doing this is via the Benjamini-Hochberg method, which controls the False Discovery Rate. FDR is the expected proportion of false positives among the results. Controlling the FDR is used to balance true results with an acceptable rate of false results. 

##### **Question 13** (SE): Why is it critical we employ a p-value adjustment here, like the FDR? 

> This is a multi-variable study, and there is a high risk of fasle positive error, purely due to chance. FDR controls this type of error, and helps the researcher avoid false discoveries, that were just purely due to chance. 

##### **Question 14** (SE): How many differentially expressed genes did we find with the **Wald test**?

>At a significance level of 0.05, (LFC > 0 = 1 gene) and (LFC < 0 = 1 gene). 1 + 1 = 2 DE genes.

##### **Question 15** (SE): Looking at the Verble et al. (2023) study, how do our results differ from their `DESeq2` results and why?

> The results differed significantly. Our results found only 2 DEGs, where the Verble paper found 371 at 10 minutes and 51 at 30 minutes. It is possible that the use of the Benjamini-Hochberg FDR method had an overly conservative threshold of 0.05. However, the more likely reason has to do with how we treated time versus how Verble treated time. We treated time as a fixed effect with treatment. If there are significant expression differences happening at specific times, having time as a fixed effect reduces the capability of detecting these differences. Whereas the Verble method tests each time point individually. This indicates that time plays a significant role in expression differences, and should be considered in analysis.   

##### Hint 1: What is different about our **design formula** versus what they describe verbally?

##### Hint 2: Look at what we did with `Time`!

# Exploratory Data Analysis

After all that hard work, it feels pretty disappointing to only find **2 genes differentially expressed** between cold-exposed and control treatments across all the time points. Let's start to visually explore the data to see if there is enough here to apply a machine-learning approach to help us out. After all, **Wald tests** are very conservative! They are also not going to do as good a job at cutting through noise, regardless of scaling & normalization.

## Summary statistics on the raw gene (feature) counts

Let's first go back to the raw data to see if there's anything diagnostic here.

```{r, echo = FALSE}
countsSummStats <- function(df, meta) {
  header <- c("Total Genes", 
              "Number Genes with >1 read count", 
              "Proportion with Nonzero Expression", 
              "Proportion with Low Expression", 
              "Number Reads: Cold-Expossed Treatment", 
              "Number Reads: Control Treatment", 
              "Total Reads Mapped", 
              "N Samples", 
              "Mean CPM per Sample")
  
  totalGenes <- nrow(df)          ## Total genes that were in the genome used for mapping
  totalCounts <- rowSums(df)
  ## Number of genes with at least 1 count:
  numNonzeroExp <- length(totalCounts[totalCounts > 0])
  propNonzeroExp <- numNonzeroExp / totalGenes
  ## How about how many genes where there are fewer than 10 reads in 90% of samples?
  lowExpression <- length(totalCounts[totalCounts < 10*ncol(df)*0.9]) 
  propLowExp <- sum(lowExpression) / totalGenes

  ## Read count by treatment
  A <- meta %>% 
    filter(Treatment == "Cold") %>% 
    rownames()
  countA <- df %>% 
    select(any_of(A)) %>% 
    sum()

  B <- meta %>% 
    filter(Treatment == "Control") %>% 
    rownames()
  countB <- df %>% 
    select(any_of(B)) %>% 
    sum()

  totalReads <- sum(df)       # Total read count
  samples <- ncol(df)         # Number of samples
  
  summStats <- as.data.frame(rbind(sprintf("%1.0f", totalGenes), 
                                   sprintf("%1.0f", numNonzeroExp), 
                                   sprintf("%1.4f", propNonzeroExp), 
                                   sprintf("%1.4f", propLowExp), 
                                   sprintf("%1.0f", countA), 
                                   sprintf("%1.0f", countB), 
                                   sprintf("%1.0f", totalReads), 
                                   sprintf("%1.0f", samples), 
                                   sprintf("%1.2f", totalReads/samples/10^6)))
  rownames(summStats) <- header
  names(summStats) <- "All Samples" 

  return(summStats)
}

summaryStatsDF <- countsSummStats(counts, metadata) 

summaryStatsDF %>% 
kable(
    format = "html",
    caption = "Table 3. Summary statistics of the raw gene expression counts") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

##### **Question 16** (SE): Look at the custom function I've written for generating the summary statistics. How is **low expression** defined?

> Low expression is defined as:
"lowExpression <- length(totalCounts[totalCounts < 10*ncol(df)*0.9])"
In this function, the number of genes with fewer than 10 reads in at least 90% of the samples is defined as low expession. 

We have nearly 40% low expression by this definition. That seems high -- but it would also make sense because these bees were put into a **chill coma** (which means their physiological processes might have shut down). In other words, the low expression might be a byproduct of the type of treatment that was applied, and could be really interesting in and of itself. Conventional methods typically **remove lowly expressed genes**, but we are doing our best at this stage to preserve all the data.

##### **Question 17** (SE): Given that it might be indicative of the study itself, do you think it could be **ill-advised** in this case to filter for low expression?

> I do think it would be ill-advised to filter for low expression in this case. As mentioned, the particular method for this study involved putting the bees into a chill coma. One of the physiological responses to being in tupor, is downregulation of gene expression. This makes sense as the metabolic system and protein synthesis is energy expensive and the bee wants to perform only the functions necessary to survive during tupor. This being the case, filtering out low expression genes may remove critical information directly relevant to understanding the bees' response to chill coma.  

## Heatmap of gene expression across treatments & time points

First, we will calculate the sample distances using a Pearson's pairwise correlation coefficient as you have typically done in your EDA to date. The `cor()` function can be applied to the matrix of normalized gene expression values and then we can make a heatmap using the `pheatmap` package. Notice that we are also applying **hierarchical clustering**, which means that we are clustering the samples by correlation coefficient. Note that to get the matrix of numeric values for input, we extract them from the VST object containing our normalized values using the `assay()` function.

#### Figure 5. Gene expression correlation among the samples with hierarchical clustering.

```{r, fig.width = 6, fig.height = 8.5, echo = FALSE}
## Compute  pairwise correlation values for samples:
pwCorr <- cor(assay(vsData))
## Let's rename the samples to something more meaningful to us using treatment and time
rownames(pwCorr) <- paste(vsData$Treatment, vsData$Time, sep="-" )
colnames(pwCorr) <- NULL        ## No column names, thanks.

## Let's set a monochromatic color palette
colors <- colorRampPalette(brewer.pal(9, "Purples"))(255)

## Let's make our heatmap!
pheatmap(pwCorr, 
         color = colors)
```

##### **Question 18** (SE): What do you notice about the arrangement of the treatments and time points (or lack thereof)?

> The arrangement of time points and treatments is unclear. There is no separation or organization of the samples based on treatment or time point. This could incorrectly lead to the indication that gene expression does not show strong correlation with condition or time points. 

##### **Question 19** (SE): Complete this code chunk to calculate the sample distances instead. You will use the `dist()`, which calculates the Euclidean distance between samples but to do it we must **transpose** the matrix first with the `t()` function. This is actually somewhat similar to what is happening in a kMeans clustering algorithm. Does your assessment about the arrangement of the treatments and time points change any?

> Yes, the arrangement is more clear here. Samples with similar gene expression are clustered together, indicating similarity in expression patterns. Profiling gene expression in this way makes it much easier to visualize. 

Hint: Make sure to update your figure caption!

```{r, fig.width = 6, fig.height = 8.5, echo = TRUE}
## Calculate the sample distances; fill in the blank and take off the comment to run
sampleDistances <- dist(t(assay(vsData)))   ## Hint: what did we do to extract a matrix from the VST object earlier?

## Make the heatmap of the sample distances; fill in any blanks and then take the comments off to run!
sampleDistMatrix <- as.matrix(sampleDistances)

## Let's label by treatment and time again
rownames(sampleDistMatrix) <- paste(vsData$Treatment, vsData$Time, sep="-" )
colnames(sampleDistMatrix) <- NULL

## Let's set a monochromatic color palette
colors <- colorRampPalette(rev(brewer.pal(9, "Purples")))(255)

pheatmap(sampleDistMatrix,
          clustering_distance_rows = sampleDistances,
          clustering_distance_cols = sampleDistances,
          col = colors)
```

## Principal Component Analysis (PCA)

PCA is a very commonly performed analysis in gene expression studies because it allows us visualize how well our different samples and conditions cluster together. We can also use PCA to identify which genes are likely to be more important to explain those differences, although we have not employed that here. Most importantly, given how **noisy the heatmap looks**, it would be nice to see if we can 'cut through the noise' with any other methods!

Notice that we are using a `plotPCA()` function which has been designed to work with gene expression data rather than `princomp()` or `prcomp`.

##### **Question 20** (SE): Do you think we could still use `princomp()` or `prcomp`?

> We totally could still use 'princomp()' or 'prcomp'. However, the 'plotPCA()' function is a part of the DESeq2 package and makes this much more convenient, as it is designed to be used with gene expression data. If using the 'princomp()' or 'prcomp' functions, the data would have to be preprocessed appropriately (normalization, transformation, etc.) prior to PCA. 

#### Figure 7. The first two principal components for the difference in gene expression across all samples, conditions, and time points.

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.width = 8}
#Create a PCA data frame
pca <- plotPCA(vsData, 
               intgroup = c("Treatment", "Colony", "Time"), 
               returnData = TRUE,
               ntop = 500)
percentVar <- round(100 * attr(pca, "percentVar"))

#Plot the PCA with the % variance attributable to PC1 and PC2
ggplot(pca, aes(PC1, PC2, color = Treatment, shape = Colony)) +
  geom_point(size=3, alpha = 0.85) +
  scale_color_manual(values=c("cadetblue", "#a8325e")) +
  labs(x = paste0("PC1: ",percentVar[1], "% variance"), 
       y = paste0("PC2: ",percentVar[2], "% variance"),
       title = "PCA of Gene Expression by Treatment + Colony + Time") +
  xlim(-30,30) +
  ylim(-15, 15) +
  theme_bw() +
  theme(axis.text = element_text(size = 13),
        legend.position = "right") +
  facet_wrap(~Time, nrow = 1)
```

##### **Question 21**: What percent of the variance is attributable to PC1 and PC2? At which time point(s) do you see the most separation between the treatments? Is this consistent with what Verble et al. (2023) found?

> Your answer here.

##### **Question 22**: Think about what you know about Support Vector Machines (we will review this in lecture a bit too). Do you see anything in these PCA plots that make you think that SVM might do a much better job of finding the boundary between the treatments?

> Your answer here.

### Volcano Plots

These are the most **canonical plots** (along with clustered heatmaps) for visualizing the results of gene expression analyses. On the y-axis is the $-log_{10}$ p-value and the x-axis is the $log_2$-fold change. The purple lines show where $log_2$-fold change between the treatment conditions is $\geq 2$, which is would mean that expression is $2^2$ times greater in one condition over the other in linear change. Note that I have used `ggrepel()` to annotate as many of the points as possible with a $log_2$-fold change of greater than 2.

#### Figure 8. Volcano plot of gene expression between cold-exposed and control bumblebees.

```{r, warning = FALSE, echo = FALSE, message = FALSE, fig.height=6, fig.width=8}
res <- resultsDESeq %>% 
  as.data.frame() %>% 
  mutate(DEG = ifelse(log2FoldChange > 0 & padj < 0.05, "Up DEG",
                       ifelse(log2FoldChange < 0 & padj < 0.05, "Down DEG", "N.S."))) %>% 
  drop_na()    ## We have to drop the NAs for Volcano plots, unfortunately!

# Next, we would like to annotate anything with greater then 2 log-fold change in expression:
res <- res %>% 
  mutate(locus = ifelse(abs(log2FoldChange) > 3, rownames(res), ""))

res %>% 
  ggplot(aes(x = log2FoldChange, y = -log10(pvalue), label=locus, color = DEG)) + 
  geom_point(alpha = 0.85, size = 1.5) +
  scale_color_manual(values=c("cadetblue", "gray", "#a8325e")) +
  geom_text_repel(show.legend = FALSE) +
  labs(y = expression(paste(-log[10], " p-value")),
       x = expression(paste(log[2], "-fold change"))) +
  theme_classic() +
  theme(axis.text = element_text(size = 12), 
        axis.title = element_text(size = 12),
        title = element_text(size = 15),
        legend.text = element_text(size = 12)) +
  geom_vline(xintercept = c(-2, 2), col="purple")                      ## Log-fold change of 2 times
#  geom_hline(yintercept = -log10(0.05), col="darkorange")             ## Corresponds to a p-value cutoff of 0.05; original not padj though, do not display
```

##### **Question 23**: Is there anything you notice from the volcano plot? Does anything about this support that a machine learning approach might perform better?

> Your answer here.

\newpage

# Machine Learning Analyses

Thus far, you have probably been exposed to the **kitchen sink** model of algorithm selection. That's pretty common. In some fields, however -- like biology and biomedicine -- folks can sometimes be a little more hesitant to just 'throw anything and everything' at their data. Why?

-   It contrasts fundamentally with how biologists think about performing **"science"**.
-   They want clear, methodologies that they can understand (transparency is key).
-   The statistics involved were not taught to them.
-   There is a very real (and sometimes justified) fear over [p-hacking](%22https://royalsocietypublishing.org/doi/10.1098/rsos.220346%22).
-   Every good analysis requires clearly justifed choices.

All of this is to say that there is nothing **wrong** with the conventional approaches -- when they work! I've published several times using `DESeq2` and I will continue to do so. So, why would I even consider switching?

One word: **NOISE**.

## Random Forest

Random Forest (RF) is a popular supervised learning algorithm, or learner, with few assumptions about the relationships among variables. RF is well-suited to capture complex interactions such as those commonly seen in biological systems, and can run either a regression or classification.

It is based on the binary decision tree, which progressively splits the samples into two child nodes that maximize how much of the variance in the dependent variable is explained by the predictors. However, single decision trees tend to over fit the data, i.e., **they tend to generate much better predictions on the training set than on the test set**. Thus, RF uses an ensemble of trees (a forest!) to counteract this overfitting (although note that it NO algorithm is robust to overfitting!). Each tree in the forest uses a random sample of the observations, while only a subset of all features (columns) is assessed for each node split.

Although sometimes called a 'black box' algorith, RF is more explainable than other deep-learning methods. One thing RF can give you is a scoring of features according to their influence; this is a measure of how **important** the feature was in generating the trees in the forest. The result is a ranked list of features that were important for predicting the outcome.

RF is being used increasingly with gene expression studies because, as an ensemble learning algorithm, it is thought to be useful in what is often referred to as the "large $P$ small $n$" paradigm, where $P$ is the number of predictors and $n$ is the number of observations. You may not fully realize it, but we have just such a $P >> n$ problem here: $n$ is actually the number of **samples** whereas $P$ is the number of genes!

So, let's see how a Random Forest performs out-of-the-box (OOB) on our data with minimal pre-processing, filtering, and no feature selection.

#### 1. Pre-processing the data.

This time through, we are doing very minimal pre-processing. We must first extract and transpose the VST data that we got out of `DESeq2`.

```{r, echo=TRUE}
## Extract the VST data
tVSdata <- t(assay(vsData))

# We do have gene names, e.g., TRNAV-UAC that are malformed for ranger and randomForest. We will fix that before proceeding:
for (c in 1:ncol(tVSdata)) {
  colName <- colnames(tVSdata)[c]
  colName <- gsub("-", "_", colName)
  colName -> colnames(tVSdata)[c]
}
```

#### 2. We do need to put our other features (Colony, Time) and our outcome (Treatment) into the transposed matrix.

Make sure to put Treatment as the last column; that will be useful downstream in the analysis pipeline.

```{r, echo=TRUE}
df1 <- cbind(colData(dsgnObject)[1], colData(dsgnObject)[3], colData(dsgnObject)[2])       ## We don't need the size factors
tVSdata <- merge(tVSdata, df1, by = "row.names")

## The merge turns back into a dataframe and removes the sample names from the rows; let's put them back:
rownames(tVSdata) <- tVSdata[,1]
tVSdata <- tVSdata[,-1]
```

#### 3. Create the test-train partitions using the `createDataPartition()` function from the `caret` package. This allows us to split with respect to the response variable, Treatment, to ensure that it is even distributed throughout the test-train split.

You will see that, with so few samples, I am opting for a 80%-20% split.

#### Figure 9. Distribution of samples between the training and testing sets.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
## Make the train and test partitions:
ind <- caret::createDataPartition(y = tVSdata[, c("Treatment")],     ## Treatment is evenly distributed
                           p = 0.8,                                    ## % into training
                           list = FALSE)                               ## don't return a list
train <- tVSdata[ind, ]
test <- tVSdata[-ind,]

## Check the distribution of the samples:
tab1 <- train %>% 
  group_by(Treatment, Time, Colony) %>% 
  summarise(Proportion = n()/nrow(train))
#tab1
tab1 %>% 
  ggplot(aes(y = Proportion, x = Treatment, fill = Colony)) +
  geom_col(color = "black") +
  scale_fill_manual(values=c("cadetblue", "#a8325e", "goldenrod")) +
  ggtitle("Training Set") +
  facet_wrap(~Time) + 
  theme_bw()

tab2 <- test %>% 
  group_by(Treatment, Time, Colony) %>% 
  summarise(Proportion = n()/nrow(test))
#tab2
tab2 %>% 
  ggplot(aes(y = Proportion, x = Treatment, fill = Colony)) +
  geom_col(color = "black") +
  scale_fill_manual(values=c("cadetblue", "#a8325e", "goldenrod")) +
  ggtitle("Testing Set") +
  facet_wrap(~Time) + 
  theme_bw()
```

##### **Question 24**: How many features (genes) are in the train and test data sets? Do you notice anything little concerning about the train and test splits. Does adjusting the split seem to make it better, worse, or stay the same?

> Your answer here.

#### 4. Fitting an initial Random Forest model.

Let's take a look at the out-of-box (OOB) performance. **This may take your computer a minute (or three) to run. Be patient.**

```{r, echo = TRUE}
rfOOB <- randomForest::randomForest(
  Treatment ~ ., 
  data = train)

rfOOB$confusion %>% 
  kable(
    format = "html",
    caption = "Table 4. Results of the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

This is the OOB classification error.

#### Figure 10. Result of out-of-box (OOB) random forest classifier on cold-exposed vs. control changes in gene expression.

```{r, echo = FALSE, fig.height = 6}
plot(rfOOB, 
     main = "OOB Random Forest", 
     lwd = 2)
```

##### **Question 25**: Based on the plot, did the OOB manage to find complete separation between cold-exposed and control treatments?

Let's look at the results of the training set when tested against the data (called **training error**):

```{r, echo = FALSE}
pred.train.rf <- predict(rfOOB, train, type = 'response')
confMat <- caret::confusionMatrix(train$Treatment,
                pred.train.rf)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 5. OOB RF Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

Now let's look at the results of the training set when tested against the test set (a.k.a., **test error** from which we derive **accuracy**):

```{r, echo = FALSE}
pred.test.rf <- predict(rfOOB, test, type = "response")
confMat <- caret::confusionMatrix(pred.test.rf, test$Treatment)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 6. OOB RF Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

Lastly, let's extract the top **important features**:

```{r, echo=FALSE, message = FALSE, warning = FALSE}
importantRF <- rfOOB$importance     ## Store the important genes!

importantRF %>% 
  as.data.frame() %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 7. Top 10 important genes identiftied by the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

##### **Question 26**: How did the OOB model perform? Which prediction (training or testing) had the highest accuracy?

> Your answer here.

##### **Question 27**: Why is it not a good idea to look at just the accuracy?

> Your answer here.

##### **Question 28**: How many of the important features (genes) overlap with the ones from the `DESeq2` analysis with log-fold change in expression \> 1? **You must write a function to solve this.** Hint: Your function will be most flexible if it takes two parameters: (1) a log-fold change to cut off against in the `DESeq` results, and (2) a list of important genes from RF or another machine learning. Trust me, a function will make your life SO much easier!!!

> Your answer here.

```{r, echo = FALSE}
findOverlappingGenes <- function() {
  print("Fill me in!")
}

findOverlappingGenes()
```

Although the OOB accuracy isn't the worst ever, especially considering how poorly `DESeq2` performed, it still looks like we may have quite a lot of overfitting and we probably aren't getting a reliable list of important genes. We fix overfitting of a model with **cross-validation**. We are going to come back to this for **PROJECT 1**.

## Support Vector Machines (SVM)

The other machine learning algorithm we are going to apply to the gene expression data is one that has been used for quite a long time now and is becoming quite conventional in its own right: SVM, or support vector machines. This algorithm is especially handy for gene expression data because it can also perform a regression or classification and it does not require (although it can handle) the boundary between discrete classes to be linear.

Just like with RF, we will not pre-process the data heavily or filter it in any way just to get a sense of the OOB performance. Then, in **Project 1** we will continue this work.

#### 1. Pre-process the datasets

Normally, data for an SVM **must** be scaled; but we have already fed it scaled and normalized data! So we have no need to do that pre-processing here.

We also cannot give an SVM an ordered factor, so we need to fix that before moving forward. We are going to turn the ordered factors (Treatment, Colony, and Time) into unordered factors and then re-call our train-test split.

```{r, echo = TRUE}
tVSdata <- tVSdata %>% 
  mutate_if(is.ordered, factor, ordered = FALSE)

train <- tVSdata[ind, ]
test <- tVSdata[-ind, ]
```

#### 2. Run the OOB (default) SVM.

As with RF, be patient and let it run, if needed. Mine only takes \~ 20 seconds, though!

```{r, echo=TRUE, warning=FALSE, message=FALSE}
svmOOB <- svm(Treatment ~ ., 
  data = train,
  kernel = "linear",
  na.action = na.omit
)
```

```{r, echo = FALSE}
paste0("The total number of support vectors was: ", svmOOB$tot.nSV)
```

#### 3. Find the **training error**.

```{r, echo = FALSE}
pred.train.svm <- predict(svmOOB, train, type = 'response')
confMat <- caret::confusionMatrix(train$Treatment,
                pred.train.svm)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 8. OOB SVM Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

#### 4. Find the test accuracy.

```{r, echo = FALSE}
pred.test.svm <- predict(svmOOB, test, type = "response")
confMat <- caret::confusionMatrix(pred.test.svm, test$Treatment)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 9. OOB SVM Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

#### 5. Extract the top **important features**.

Unlike RF, the `svm()` function in `e1071` does not estimate the importance of the predictors for you. However, we can use a different function, `mt.teststat` from the

```{r, echo=FALSE, message=FALSE, warning=FALSE}
labels <- as.integer(train$Treatment) - 1

tTrain <- train %>% 
  select(-Colony, -Time, -Treatment) %>%
  t() %>% 
  as.matrix()

tscores <- mt.teststat(tTrain, 
                       labels, 
                       test = "t")
```

## Store the important genes!

```{r, echo = FALSE}
geneID <- rownames(tTrain)
importantsvm_1 <- data.frame(geneID, tscores)

importantsvm_1 %>% 
  arrange(desc(tscores)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 10. Top 10 important genes identiftied by the OOB SVM") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```

##### **Question 29**: Try quickly repeating these analyses with `kernel = "radial"` as well as `kernel = "sigmoid"` in Step #2 above. Follow through with your predictions - which kernel yields the lowest training error and the highest test accuracy?

> Your answer here.

##### **Question 30**: Use your function from question 28 - do you see an overlap between the `DESeq2` genes with greater than 2 log-fold expression?

> Your answer here.

##### **Question 31**: At this point, which model is performing better on the gene expression data and why?

> Your answer here.

# Final Comments

At this stage, it should feel clear that we need to do feature selection. If not, stop and reflect on that statement. We know there is a lot of **noise** in the data, and we have worked hard to preserve all of the observations and columns. However, we know that feature selection and tuning can both have DRASTIC effects on the performance of our models. Which one will your team choose to tune for **Project 1**?

**N.B.:** Did you find this module fun or interesting? Would you like to work with genes and gene expression data more deeply? Interested in a research opportunity? Talk to Dr. Geist about opportunities to work on and potentially publish benchmarking studies like this!

# References
