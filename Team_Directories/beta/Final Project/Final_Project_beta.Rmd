---
title: "Final Project: Willow"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Adeline Casali, Samantha Harper, Scott Eugley"
date: "30 June 2024"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    fig_crop: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE, 
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed; add CMRG for parallelization
set.seed(50009, "L'Ecuyer-CMRG")

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               DESeq2,
               RColorBrewer,
               gridExtra,
               miniml, 
               ggrepel,
               e1071,
               Biobase, 
               caret,
               GEOquery, 
               ROCR, 
               pROC, 
               randomForest,
               ranger,
               multtest,
               nestedcv,
               Rfast, 
               pdp, 
               vip, 
               kernlab, 
               BiocManager, 
               readr
)
```


# Introduction

# Data Processing

```{r}
#Load processed data
willow <- read_delim("./Data/GSE218490_Processed_data.txt", delim = "\t", locale = locale(encoding = "UTF-16"))

#Load Gene Annotation Data
#This shows info for each gene
GeneAnnotation <- read_delim("./Data/GSE218490_S.viminalis_root_transcriptome_V1.0.annotation.txt", 
                             delim = "\t", 
                             col_names = TRUE)

#Load MetaData
metadata <- read_csv("./Data/salix_metadata.csv")
```
```{r}
#Clean willow dataset to just include counts
#Not 100% sure which count to use, but total_contig_reads seemed to match demo 1 the most?
willow <- willow[, c("Feature_ID", "CT_R1-Total_contig_reads", "CT_R2-Total_contig_reads", "CT_R3-Total_contig_reads", "CC_R1-Total_contig_reads", "CC_R2-Total_contig_reads", "CC_R3-Total_contig_reads", "MT_R1-Total_contig_reads", "MT_R2-Total_contig_reads", "MT_R3-Total_contig_reads", "MC_R1-Total_contig_reads", "MC_R2-Total_contig_reads", "MC_R3-Total_contig_reads")]


```

```{r}
#Set Feature_ID as row numbers
rownames(willow) <- willow$Feature_ID
willow <- willow[ ,-1]
```
```{r}
rownames(metadata) <- metadata$Accession
metadata <- metadata[ ,-1]
```
```{r, results='hide', echo = TRUE}
levels(as.factor(metadata$Treatment))
metadata$Treatment <- factor(metadata$Treatment, levels = c("Control", "Cold-Acclimated Control", "Metal-Polluted", "Metal-Polluted Cold-Acclimated"))
```

```{r, echo = TRUE, results='asis'}
print(all(colnames(counts) %in% rownames(metadata)))
print(all(colnames(counts) == rownames(metadata)))
ncol(willow) == nrow(metadata)
```

```{r}
# Create DESeqDataSet object
dds <- DESeqDataSetFromMatrix(
  countData = willow,
  colData = metadata,
  design = ~ Treatment)

dim(dds)
```

# Random Forest

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Do Splits from Project 1
#So we don't have to reinvent the wheel here
doSplits <- function(vst, algorithm, splitRatio, filterCutoff) {
  ### @vst = vst dataset as extracted from DESeq2
  ### @algorithm = ML algorithm used; currently set up for rf and svm
  ### @splitRatio = the split ratio to employ (training size)
  ### @filterCutoff = the filter cutoff for median number of VST gene counts
  
  ## According to the Valabas et al. (2019) paper, make sure that we are filtering in TRAINING set only! 

  # Extract the VST data and transpose
  tVST <- t(assay(vst))
  
  # We do have gene names, e.g., TRNAV-UAC that are malformed for ranger and randomForest. We will fix that before proceeding:
  for (c in 1:ncol(tVST)) {
    colName <- colnames(tVST)[c]
    colName <- gsub("-", "_", colName)
    colName -> colnames(tVST)[c]
  }
  
  ## Add the metadata as columns & merge
  df1 <- cbind(colData(vst)[1], colData(vst)[3], colData(vst)[2])       ## We don't need the size factors
  tVST <- merge(tVST, df1, by = "row.names")

  ## The merge turns back into a dataframe and removes the sample names from the rows; let's put them back:
  rownames(tVST) <- tVST[,1]
  tVST <- tVST[,-1]
  
  if(algorithm == "svm") {
    ## Make the factors unordered
    tVST <- tVST %>% 
      mutate_if(is.ordered, factor, ordered = FALSE)
  }
  
  ## Create the data partitions
  ind <- createDataPartition(y = tVST[, c("Treatment")],     ## Treatment is evenly distributed
                             p = splitRatio,                    ## % into training
                             list = FALSE)                      ## don't return a list
  train <- tVST[ind, ]
  test <- tVST[-ind,]
  
  ## Now apply the filtering:
  # Calculate row medians of VST gene counts
  medians <- rowMedians(assay(vst))

  # Filter the features out of train:
  train <- train[, medians > filterCutoff]  
  print(paste0("After filtering, the number of genes remaining in the dataset are: ", ncol(train)))

  splits <- list(train, test)
  return(splits)
}
```


```{r}
#Split data
#Do before processing to avoid data leakage

#Extract data from design object
#Be sure to check filter cutoff

```

```{r}
#
```



# Results and Discussion

# Conclusion