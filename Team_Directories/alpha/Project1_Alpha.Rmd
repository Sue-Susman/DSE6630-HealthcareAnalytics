---
title: 'Project 1: Bioinformatics of Gene Expression'
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Ryan Canfield, Malek Sabri, Timothy Shaffer"
date: "2025-5-20"
output: word_documeny
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed; add CMRG for parallelization
set.seed(50009, "L'Ecuyer-CMRG")

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               RColorBrewer,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest,
               nestedcv,
               Rfast
)
```




# Questions
##### **Question 1**: Look up these rules and try to explain what they are doing for yourself. 
> Your answer here.


##### **Question 2**: Take a moment to go look at the function and the documentation I have written. Also try playing with the parameters a bit - how many more or fewer genes do you get when you lower or raise the `filterCutoff`, respectively?
> Your answer here.


##### **Question 3**: Do some research on __data leakage__ and try to explain what that means in this context. Why do we care about data leakage?
> Your answer here.


##### **Question 4**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> Your answer here.


##### **Question 5**: Can you see an impact of the pre-processing (to remove lowly expressed genes)?
> Your answer here.


##### **Question 6**: Can you see an impact of the 10-fold cross-validation?
> Your answer here.


##### **Question 7**: How many different combinations are we going to search with this grid?
> Your answer here.


##### **Question 8**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> Your answer here.


##### **Question 9**: How well did hyperparameter tuning with the grid search perform?
> Your answer here.


##### **Question 10**: Use the `modelLookup` function to help you figure out what hyperparameters are available for the `rf` vs. `ranger` methods for performing random forest algorithm with `caret`. What are the key differences you observce in the hypers you can tune?
> Your answer here.


##### **Question 11**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.) How does it compare to previous results?
> Your answer here.


##### **Question 12**: How well did hyperparameter tuning with the grid search perform?
> Your answer here.


##### **Question 13**: Notice that the y-axis is now **log-Loss** and not **accuracy**. You may need to do some digging, but can you figure out why? Sources can be helpful if you're not sure you're on the right track!
> Your answer here.


##### **Question 14**: The ROC and precision-recall curves are suggesting very high accuracy. Woohoo!!... Right?! What is the confusion matrix telling us? How do you know?
> Your answer here.


##### **Question 15**: Should we have any confidence in these results? Why or why not?
> Your answer here.


# Background & Questions


# Data Source


# Analysis


# Conclusions


# Recommendations and Next Steps

