---
title: 'Project 1: Bioinformatics of Gene Expression'
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Ryan Canfield, Malek Sabri, Timothy Shaffer"
date: "2025-5-20"
output: word_documeny
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed; add CMRG for parallelization
set.seed(50009, "L'Ecuyer-CMRG")

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               RColorBrewer,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest,
               nestedcv,
               Rfast
)
```




# Questions
##### **Question 1**: Look up these rules and try to explain what they are doing for yourself. 
> The gini rule is a measure of randomness in a dataset's target variable. The gini rule is calculated for a given node by considering the proportion of each class in the node. The extratrees rule creates many decision trees, but the sampling for each tree is random, without replacement. This creates a dataset for each tree with unique samples.


##### **Question 2**: Take a moment to go look at the function and the documentation I have written. Also try playing with the parameters a bit - how many more or fewer genes do you get when you lower or raise the `filterCutoff`, respectively?
> Your answer here.


##### **Question 3**: Do some research on __data leakage__ and try to explain what that means in this context. Why do we care about data leakage?
> In this context, data leakage means where information from outside the training dataset is inadvertently used to create a machine learning model. We care about data leakage because it can mislead the modelâ€™s performance, invalidate conclusions, and generalize issues. 


##### **Question 4**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> Initially the gini splitting rule has the highest accuracy until the randomly selected predictors hit around 6800 then extratrees has the highest accuracy.


##### **Question 5**: Can you see an impact of the pre-processing (to remove lowly expressed genes)?
> I can not see an impact of the pre-processing.


##### **Question 6**: Can you see an impact of the 10-fold cross-validation?
> I'd say that you can see an impact fromt the 10-fold CV. You can see this in figure 1. It shows the increased accuracy for extratees and decreased accuracy for gini. Then at around 6800 randomly selected predictors there is an accuracy overlap. The good thing about the 10-fold CV is that we are reducing overfitting along with it doing automated hyperparameter tuning and thus must be increasing accuracy = Malek Sabri


##### **Question 7**: How many different combinations are we going to search with this grid?
> This grid will utilize the 5 values for mtry, the 2 values for splitrule, and the 4 values for min.node.size leading me to believe there could be 40 combinations = Malek Sabri


##### **Question 8**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> When the min.node.size is 5, splitrule is extratees, and mtry is 23 I see the highest accuracy = Malek Sabri


##### **Question 9**: How well did hyperparameter tuning with the grid search perform?
> Based on the confusion matrices there is not an improvement and thus the tuning was not useful when being utilized with the grid search = Malek Sabri


##### **Question 10**: Use the `modelLookup` function to help you figure out what hyperparameters are available for the `rf` vs. `ranger` methods for performing random forest algorithm with `caret`. What are the key differences you observce in the hypers you can tune?
> The key differences in the tuning between rf and ranger is that ranger allows for more tuning capabilites. It appears that rf can tune mtry, but ranger can also tune min.node.size and splitrule = Malek Sabri


##### **Question 11**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.) How does it compare to previous results?
> Your answer here.


##### **Question 12**: How well did hyperparameter tuning with the grid search perform?
> Your answer here.


##### **Question 13**: Notice that the y-axis is now **log-Loss** and not **accuracy**. You may need to do some digging, but can you figure out why? Sources can be helpful if you're not sure you're on the right track!
> Your answer here.


##### **Question 14**: The ROC and precision-recall curves are suggesting very high accuracy. Woohoo!!... Right?! What is the confusion matrix telling us? How do you know?
> Your answer here.


##### **Question 15**: Should we have any confidence in these results? Why or why not?
> Your answer here.


# Background & Questions


# Data Source


# Analysis


# Conclusions


# Recommendations and Next Steps

