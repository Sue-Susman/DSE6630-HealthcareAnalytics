---
title: 'Project 1: Bioinformatics of Gene Expression'
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Ryan Canfield, Malek Sabri, Timothy Shaffer"
date: "2025-5-20"
output: word_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed; add CMRG for parallelization
set.seed(50009, "L'Ecuyer-CMRG")

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               RColorBrewer,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest,
               nestedcv,
               Rfast
)
```




# Questions
##### **Question 1**: Look up these rules and try to explain what they are doing for yourself. 
> The gini rule is a measure of randomness in a dataset's target variable. The gini rule is calculated for a given node by considering the proportion of each class in the node. The extratrees rule creates many decision trees, but the sampling for each tree is random, without replacement. This creates a dataset for each tree with unique samples.


##### **Question 2**: Take a moment to go look at the function and the documentation I have written. Also try playing with the parameters a bit - how many more or fewer genes do you get when you lower or raise the `filterCutoff`, respectively?
> Your answer here.


##### **Question 3**: Do some research on __data leakage__ and try to explain what that means in this context. Why do we care about data leakage?
> In this context, data leakage means where information from outside the training dataset is inadvertently used to create a machine learning model. We care about data leakage because it can mislead the modelâ€™s performance, invalidate conclusions, and generalize issues. 


##### **Question 4**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> Initially the gini splitting rule has the highest accuracy until the randomly selected predictors hit around 6800 then extratrees has the highest accuracy.


##### **Question 5**: Can you see an impact of the pre-processing (to remove lowly expressed genes)?
> I can not see an impact of the pre-processing.


##### **Question 6**: Can you see an impact of the 10-fold cross-validation?
> I'd say that you can see an impact fromt the 10-fold CV. You can see this in figure 1. It shows the increased accuracy for extratees and decreased accuracy for gini. Then at around 6800 randomly selected predictors there is an accuracy overlap. The good thing about the 10-fold CV is that we are reducing overfitting along with it doing automated hyperparameter tuning and thus must be increasing accuracy = Malek Sabri


##### **Question 7**: How many different combinations are we going to search with this grid?
> This grid will utilize the 5 values for mtry, the 2 values for splitrule, and the 4 values for min.node.size leading me to believe there could be 40 combinations = Malek Sabri


##### **Question 8**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.)
> When the min.node.size is 5, splitrule is extratees, and mtry is 23 I see the highest accuracy = Malek Sabri


##### **Question 9**: How well did hyperparameter tuning with the grid search perform?
> Based on the confusion matrices there is not an improvement and thus the tuning was not useful when being utilized with the grid search = Malek Sabri


##### **Question 10**: Use the `modelLookup` function to help you figure out what hyperparameters are available for the `rf` vs. `ranger` methods for performing random forest algorithm with `caret`. What are the key differences you observce in the hypers you can tune?
> The key differences in the tuning between rf and ranger is that ranger allows for more tuning capabilites. It appears that rf can tune mtry, but ranger can also tune min.node.size and splitrule = Malek Sabri


##### **Question 11**: What combination of hyperparameters gives the highest accuracy in the **training model**? (Use plot above to answer.) How does it compare to previous results?
> When using 50 trees you get the highest accuracy when you use 10, or 20 nodes, and 1919 mtry or number features randomly sampled at each split. The accuracy is around 98~99% on when training the model. If you are using 200 trees you want to use either 5 or 20 nodes, and still the 1919 mtry. This will also give you an accuracy of 98~99%. This is similar to the results of the Grid Search on Random Forest since it also found that 5 nodes and 1919mtry were the best hyperparameters. --Ryan Canfield


##### **Question 12**: How well did hyperparameter tuning with the grid search perform?
> The hyperparameter tuning with the grid search preformed exactly the same as with the other grid search both got eactly 57%. I think this is because the new tuned model has a node size of 1 which turns the descions tree into a binary answer which is why we would be getting the low accuracy. -- Ryan Canfield 


##### **Question 13**: Notice that the y-axis is now **log-Loss** and not **accuracy**. You may need to do some digging, but can you figure out why? Sources can be helpful if you're not sure you're on the right track!
> While researching I found that when evaluating the performance of a model using log-loss instead of accuracy, it is because the model is now trained on probabilistic predictions. An example of models with probability outputs are  logistic regression or random forests. Finally, the website says its just more a comprehensive evaluation of model performance when the outputs are probabilities not numbers. -- Ryan Canfield

> https://datascience.stackexchange.com/questions/39825/log-loss-vs-accuracy-for-deciding-between-different-learning-rates 
> https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d#:~:text=Log%20Loss%20is%20similar%20to,reference%20values%20to%20compare%20it.

##### **Question 14**: The ROC and precision-recall curves are suggesting very high accuracy. Woohoo!!... Right?! What is the confusion matrix telling us? How do you know?
> The confusion matrix is on the testing data set and contains 14 data points 7 of each points. 6 are correctly labeled a control point and 7 are correctly labeled cold point. the last point was miscorrectly labeled a cold point when it was actually labeled a control point. This type of error is a Type II error or False Negative. I know this the accuracy is accurate because it was taken from the output based off of the confusion matrix. 13/14 ~ 92%.


##### **Question 15**: Should we have any confidence in these results? Why or why not?
> I think we should have confidence because the repeated appearance of LOC100747964 suggests it could have potential for being biologically significant. This is just and inference and not a fact, I think it is important to build other models and test their validation their answers so we can have a more reliable way to confirm it's the gene's relevance. 

# Background & Questions


# Data Source


# Analysis


# Conclusions


# Recommendations and Next Steps




