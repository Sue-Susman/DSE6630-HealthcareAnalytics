---
title: "Project 2: Biomedical & Clinical Informatics"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Ryan Canfield, Malek Sabri, Tiffany Acosta"
date: "30 April 2024"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    fig_crop: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(50009)

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse,
               janitor,
               naniar,
               stringr,
               ggplot2, 
               kableExtra,
               RColorBrewer,
               gridExtra,
               gtsummary,
               prcomp,
               factoextra,
               ggrepel,
               caret,
               missForest,
               mice,
               car,
               stargazer,
               glmnet,
               forcats
)
```

# 1 Introduction

#### Adventure 1. [Minimal coding, focus is on understanding cleaning, pre-processing, and the ML analyses we performed here.]
Choose ONE other condition (anything other than pneumonia) and run through the analyses again, using our same target variable. You will update your question, hypotheses, and predictions, but will be able to re-use all of the other code with very minor modifications. Credit will be based more on interpretation and not on coding.


For **Project 2**, you will have the option to **modify** and **run** the code in the `answers_demo2.R` file and run it in your markdown file, as I do below. Now, you don't have to run the code right now unless you want to see it in action; if you do, make sure to give it a couple minutes to run, as it's doing a lot in one fell swoop. Or, you can just skip ahead to the next section and load the data. 

```{r, echo = FALSE, warning = FALSE, message=FALSE}
# Remove the comments to run.

## This calls the code in the associated .R file
source(file = "answers_demo2_Alpha_HF.R", echo = FALSE)
```

## 1.4 Load data from the end of the demo.
Load my version of the fully cleaned, encoded, & ready-to-proceed dataset from the end of Demo 2. These data are not yet ready for analysis (!) but we are picking up with that here.

```{r}
load("HEART_FAILURE_AnalyzefromDemo.Rdata")

```

# 2 Exploratory Data Analysis (Continued)

```{r, echo = FALSE, collapse=TRUE}
dict <- tribble(
~`Possible Target`, ~`Description`,
"`ComparedToNational_Hospital return days for heart failure patients`", "Hospital return days measures the number of days patients spent back in the hospital (in the emergency department, under
observation, or in an inpatient unit) within 30 days after they were first treated and released for heart failure. Reported as compared to the national average, such that 'below average' is better and 'above average' is worse.", 
"`ExpectedReadmissionRate`", "The expected number of readmissions in each hospital is estimated using its patient mix and an average hospital-specific intercept. It is thus indirectly standardized to other hospitals with similar case and patient mixes.",
"`PredictedReadmissionRate`", "The number of readmissions within 30 days predicted based on the hospitalâ€™s performance with its observed case mix. The predicted number of readmissions is estimated using a hospital-specific intercept, and is intended to reflect the annual expected performance of the hospital given its historical case and patient mix and performance.",
"`ExcessReadmissionRatio`", "The ratio of the predicted readmission rate to the expected readmission rate, based on an average hospital with similar patients. Performance is compared against a ratio of one, such that below one is better and above one is worse in terms of readmission rates.",
"`observed_readmission_rate`", "Our calculation of the observed number of heart failure-related readmissions within 30 days found by dividing the number of readmissions observed for the hospital during the given period by the number of discharges for that same period, and multiplied by 100 to put it on the same scale as the Predicted and Expected Readmission Rates. It reflects a true observation as reported by the hospital during that period, but is not adjusted for case mixes or prior information for that hospital. Thus, this is a crude, unadjusted value."
)

dict %>% 
  kable(
    format = "html",
    caption = "Table 1. List of possible target variables from the Heart failure-related hospital readmission data. Which one to choose?") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```


```{r, echo = FALSE}
sliceData <- HEART_FAILURE_AnalyzeDemo %>% 
  select(`ComparedToNational_Hospital return days for heart failure patients`, 
         PredictedReadmissionRate,
         observed_readmission_rate, 
         ExcessReadmissionRatio,
         ExpectedReadmissionRate) %>% 
  rename(`National Comparison Return Hospital Days` = `ComparedToNational_Hospital return days for heart failure patients`,
         `Predicted Readmission Rate` = PredictedReadmissionRate, 
         `Observed Readmission Rate` = observed_readmission_rate, 
         `ExcessReadmissionRatio` = ExcessReadmissionRatio,
         `Expected Readmission Rate` = ExpectedReadmissionRate) %>% 
  mutate(`National Comparison Return Hospital Days` = ifelse(is.na(`National Comparison Return Hospital Days`), "Unknown", 
                                                      ifelse(`National Comparison Return Hospital Days` == 1, "Better than average",
                                                      ifelse(`National Comparison Return Hospital Days` == 0, "Same as average", "Worse than average")))) %>% 
  mutate(`National Comparison Return Hospital Days` = factor(`National Comparison Return Hospital Days`, levels = c( "Better than average", "Same as average", "Worse than average", "Unknown")))

sliceData %>% 
  tbl_summary(statistic = list(
      all_continuous() ~ "{mean} ({sd}), {median}",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ), missing_text = "(Missing)") %>% 
  modify_caption("**Table 2. Possible target measure summaries**") %>%
  bold_labels()
```


```{r, echo = FALSE}
## Make the long-format version of just the possible target variables, with some cleanup of the names, etc. for nicer graphs
longDataTarget <- HEART_FAILURE_AnalyzeDemo %>%
  ## Make a shorter name
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for heart failure patients`) %>% 
  ## Relabel the categories so we know which is which; not necessary but makes things easier
  mutate(NatlComparisonHospitalDays = ifelse(NatlComparisonHospitalDays == 1, "Better", 
                                   ifelse(NatlComparisonHospitalDays == -1, "Worse", 
                                   ifelse(NatlComparisonHospitalDays == 0, "Same", NatlComparisonHospitalDays)))) %>% 
  ## Grab just the five features we want so we can compare them
  select(contains(c("readmission", "NatlComparisonHospitalDays"))) %>% 
  ## Pivot longer
  pivot_longer(-c(5), names_to = "Variable", values_to = "Value") %>% 
  ## Rename to make the graphed labels prettier; not the only way to do this, but it's my preferred way of doing it
      mutate(NatlComparisonHospitalDays = ifelse(is.na(NatlComparisonHospitalDays), "Unknown", NatlComparisonHospitalDays),
         Variable = ifelse(Variable == "PredictedReadmissionRate", "Predicted Readmission Rate",
                    ifelse(Variable == "ExcessReadmissionRatio", "Excess Readmission Ratio",
                    ifelse(Variable == "observed_readmission_rate", "Observed Readmission Rate", "Expected Readmission Rate")))) %>%
      mutate(NatlComparisonHospitalDays = factor(NatlComparisonHospitalDays, levels = c("Better", "Same", "Worse", "Unknown")))
```


#### Example with `geom_violin()`:
Here I am leaving the `NA` data (which I relabelled as `Unknown`) to get a sense for the degree of missingness.
```{r, warning = FALSE, message = FALSE, echo = FALSE}
longDataTarget %>% 
  ggplot(aes(x = Value, y = NatlComparisonHospitalDays, fill = NatlComparisonHospitalDays)) +
  geom_violin(alpha = 0.7) +
  labs(title = "Which target to use? National comparison of readmitted hospital days.",
        x = "",
        y = "Rate or Score",
        fill = "Heart Failure-related hospital days (compared to national average):") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon", "darkgreen"),
                    labels = c("Better", "Same", "Worse", "Unknown")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=9),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

#### Example with `geom_density()`:
However, this time I am going to ignore the "Unknown" (i.e., missing) class of 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
longDataTarget %>% 
  filter(NatlComparisonHospitalDays != "Unknown") %>% 
  ggplot(aes(x = Value, fill = NatlComparisonHospitalDays)) +
  geom_density(alpha = 0.45) +
  labs(title = "Which target to use? National comparison of readmitted hospital days.",
        x = "",
        y = "Rate or Score",
        fill = "Heart Failure-related hospital days (compared to national average):") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon", "darkgreen"),
                    labels = c("Better", "Same", "Worse", "Unknown")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=9),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

### 2.3 What is the distribution of various hospital metrics across heart failure-related readmission as compared to the national average?
```{r, fig.width=6, fig.height=8, echo = FALSE}
longDataScore <- HEART_FAILURE_AnalyzeDemo %>%
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for heart failure patients`) %>% 
  select(contains(c("Score", "NatlComparisonHospitalDays", "ExpectedReadmissionRate"))) %>% 
  pivot_longer(-c(3, 16, 17), names_to = "Variable", values_to = "Value") %>% 
  drop_na() %>% 
  mutate(Variable = gsub("Score_", "", Variable)) %>% 
  mutate(Variable = ifelse(Variable == "Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better", "Median time (minutes) patients spent in ED",
                    ifelse(Variable == "CMS Medicare PSI 90: Patient safety and adverse events composite", "Composite patient safety", 
                    ifelse(Variable == "Abdomen CT Use of Contrast Material", "Abdomen CT w/ contrast", 
                    ifelse(Variable == "Perioperative pulmonary embolism or deep vein thrombosis rate", "Perioperative pulmonary embolism/DVT",                     
                    ifelse(Variable == "Percentage of healthcare personnel who completed COVID-19 primary vaccination series", "Healthcare workers given 1st COVID-19 vaccine",           ifelse(Variable == "Left before being seen", "% left ED before being seen", 
                    ifelse(Variable == "Intensive Care Unit Venous Thromboembolism Prophylaxis", "Venous Thromboembolism Prophylaxis in ICU", Variable))))))),
         NatlComparisonHospitalDays = ifelse(NatlComparisonHospitalDays == 1, "Better", 
                                      ifelse(NatlComparisonHospitalDays == -1, "Worse",
                                      ifelse(NatlComparisonHospitalDays == 0, "Same", NatlComparisonHospitalDays))))

longDataScore %>%  
  ggplot(aes(y = Value, 
             x = as.factor(NatlComparisonHospitalDays), 
             fill = as.factor(NatlComparisonHospitalDays))) +
    geom_violin(scale = "width", alpha = 0.7) + 
    labs(title = "How does heart failure-related readmission differ across \nvarious hospital scores or rates?",
        x = "",
        y = "Rate or Score",
        fill = "Heart Failure-related Readmission Compared to National Average") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon"), 
                    labels = c("Better", "Same", "Worse")) + 
  theme_minimal() +
  theme(legend.position = "bottom",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        strip.text = element_text(size=8),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

### 2.4 What is the relationship of heart failure-related hospital readmissions to the various hospital metrics?
```{r, warning=FALSE, message = FALSE, fig.width=6, fig.height=8, echo = FALSE}
longDataScore %>%  
  ggplot(aes(y = ExpectedReadmissionRate, 
             x = Value)) +
    geom_point(alpha = 0.25) + 
    geom_smooth(aes(fill = as.factor(NatlComparisonHospitalDays), color = as.factor(NatlComparisonHospitalDays)), 
                method = "lm", 
                se = T, 
                show.legend = T) + 
    labs(title = "Relationship of Heart Failure-related hospital readmissions to each the various hospital metrics",
        x = "Rate or Score",
        y = "Expected Heart Failure-related Readmissions",
        color = "Heart Failure-related Readmission Compared to National Average") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon")) + 
  scale_color_manual(values = c("gold", "cadetblue", "maroon"),
                    labels = c("Better", "Same", "Worse")) + 
  guides(fill = "none", color = guide_legend(override.aes = list(fill=NA))) +    ## To override the filled SE boxes from geom_smooth() in the legend
  theme_minimal() +
  theme(legend.position = "bottom",
        # axis.text.x = element_blank(),
        # axis.ticks.x = element_blank(),
        strip.text = element_text(size=8),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free_x",
             ncol = 2)
```

##### **Question 1**: [1 point]
Use the code above, or write your own *de novo*, to explore the relationships between the expected heart failure  readmissions and the `HcahpsLinear...` variables.
```{r, fig.width=6, fig.height=8, echo = TRUE, message = FALSE}
# #Malek
hcahps_data <- HEART_FAILURE_AnalyzeDemo %>%
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for heart failure patients`) %>% 
  select(contains(c("NatlComparisonHospitalDays", "ExpectedReadmissionRate", "HcahpsLinear"))) %>% 
  pivot_longer(-c(1, 2, 12), names_to = "Variable2", values_to = "Value2") %>% 
  drop_na() %>% 
  mutate(Variable2 = gsub("Score_", "", Variable2)) %>% 
  mutate(Variable2 = ifelse(Variable2 == "Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better", "Median time (minutes) patients spent in ED",
                    ifelse(Variable2 == "CMS Medicare PSI 90: Patient safety and adverse events composite", "Composite patient safety", 
                    ifelse(Variable2 == "Abdomen CT Use of Contrast Material", "Abdomen CT w/ contrast", 
                    ifelse(Variable2 == "Perioperative pulmonary embolism or deep vein thrombosis rate", "Perioperative pulmonary embolism/DVT",                     
                    ifelse(Variable2 == "Percentage of healthcare personnel who completed COVID-19 primary vaccination series", "Healthcare workers given 1st COVID-19 vaccine",           ifelse(Variable2 == "Left before being seen", "% left ED before being seen", 
                    ifelse(Variable2 == "Intensive Care Unit Venous Thromboembolism Prophylaxis", "Venous Thromboembolism Prophylaxis in ICU", Variable2))))))),
         NatlComparisonHospitalDays = ifelse(NatlComparisonHospitalDays == 1, "Better", 
                                      ifelse(NatlComparisonHospitalDays == -1, "Worse",
                                      ifelse(NatlComparisonHospitalDays == 0, "Same", NatlComparisonHospitalDays))))


hcahps_data %>%  
  ggplot(aes(y = ExpectedReadmissionRate, 
             x = Value2)) +
    geom_point(alpha = 0.25) + 
    geom_smooth(aes(fill = as.factor(NatlComparisonHospitalDays), 
                    color = as.factor(NatlComparisonHospitalDays)), 
                method = "lm", 
                se = TRUE, 
                show.legend = TRUE) + 
    labs(title = "Relationship of Heart Failure-related hospital expected readmissions to HcahpsLinear variables",
         x = "Rate or Score",
         y = "Expected Heart Failure-related Readmissions",
         color = "Expected heart failure readmissions Compared to National Average") +
    scale_fill_manual(values = c("gold", "cadetblue", "maroon")) + 
    scale_color_manual(values = c("gold", "cadetblue", "maroon"),
                       labels = c("Better", "Same", "Worse")) + 
    guides(fill = "none", color = guide_legend(override.aes = list(fill = NA))) + 
    theme_minimal() +
    theme(legend.position = "bottom",
          strip.text = element_text(size = 8),
          legend.text = element_text(size = 8),
          legend.title = element_text(size = 10),
          title = element_text(size = 12, color = "maroon")) + 
    facet_wrap(~ Variable2, 
               scales = "free_x",
               ncol = 2)
```

##### **Question 2**: [1 point]
What relationships do you find most compelling from your graphic and why? Also, name one outstanding question you have at this point, whether it is about the data, the data collection, or something more speculative.

> #Malek
Here I would not say that there is one specific relationship that stands out the most. The biggest question that I have as of now is that I wonder if the relationship of the variables is similar based on the size or geographical location of the hospital.

#### Bias and Limitation

##### **Question 3**: [1 point]
All studies have bias (introduced or systemic error) and limitations (failure to fully explain something). We could go more deeply into these concepts, but for now I want you to take a stab and just brainstorming either ways we might have bias OR the limitations of using this target variable. You do not need to answer both, but you're welcome to do so. You can read more about bias [here](https://www.ncbi.nlm.nih.gov/books/NBK574513/) or see some deeper explanations of limitations [here](https://www.aje.com/arc/how-to-write-limitations-of-the-study/). You do not need to write a lot; just 2-3 sentences. I am simply asking you to pause and reflect on our choices here before we proceed. 

> #Malek
This answer is pretty much identical to the one from the first half of this project. One potential bias in using the Expected readmission rate as our target variable is selection bias. The thing that is making me weary here is that the data is pretty reliant on historical data which may or may not be up to date. Thus, if one hospital has more data than another that is really covering all the bases then I believe this would definitely be shown in the outcomes. 


# 3 Pre-processing and Feature Selection

### 3.1.1 Dropping the targets we aren't planning to use. [Manual Method]
```{r}
## Clean up the column names quickly - remove "Score_", "HcahpsLinearMeanValue_", "_Payment"
colnames(HEART_FAILURE_AnalyzeDemo) <- gsub('Score_|HcahpsLinearMeanValue_|_Payment', '', colnames(HEART_FAILURE_AnalyzeDemo))
```

##### **Question 4**: [1 point]
Why do you think I choose to keep `Hospital return days for pneumonia patients` and `ComparedToNational_Hospital return days for pneumonia patients`? Do you agree with my choice? Why or why not? What analytic pitfalls are there to including them?

> #Malek
Again, this answer is going to be pretty similar to part 1. I would say that I do agree with your choice... Probably always best to agree with you in these situations as there is likely a reason why you did what you did haha. Anyways, I like keeping both of them in there as it gives some kind of standard for us to use and compare with. As long as things aren't becoming redundant, which I don't believe they are in this case, then we should be in good shape. In terms of the new target variable, I do believe that this is still useful, but just on the heart failute side as these two pieces of data can be useful when comparing to eachother. Especially because of the many factors that go into heart failure this can be a good way to see regional differences compared with national. 


### 3.1.2 Dropping any features with near-zero variance. [Automated Method]
```{r, echo = F}
## Identify the columns with near-zero variance
zero_var_df <- HEART_FAILURE_AnalyzeDemo[, nearZeroVar(HEART_FAILURE_AnalyzeDemo)]

## Print the columns
data.frame(names(zero_var_df)) %>% 
  rename(Variable = names.zero_var_df.) %>% 
  kable(
    format = "html",
    caption = "Table 3. Columns dropped due to near-zero variance.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)

## Drop them from the dataset:
HEART_FAILURE_AnalyzeDemo <- HEART_FAILURE_AnalyzeDemo[, -nearZeroVar(HEART_FAILURE_AnalyzeDemo)]
```

##### **Question 5**: [1 point]
If you are not familiar yet with the concept of __zero or near-zero variance__, do a little digging. Based on what you read, try to explain for your naive advocacy group stakeholders, why it's important to remove the features with little variance.

> #Malek
In this instance, the data points that got dropped were PaymentCategory for heart failure patients, ComparedToNational_Postoperative respiratory failure rate, and ComparedToNational_Perioperative pulmonary embolism or deep vein thrombosis rate. So, zero or near-zero variance deals with data that is pretty much exactly the same and thus the information is redundant and not useful. The problem arises when trying to build our models and all this data is doing is creating noise. Ultimately making our models less helpful. When these are removed the models are better able to focus on the important things. 


##### **Question 6**: [1 point]
Why do I want to make sure unique identifiers, like `FacilityID`, are out of the dataframe for before I either run `nearZeroVar()` or proceeding with the rest of the analysis? What problems does keeping them in there create?

>#Malek 
Unique identifiers need to get removed because they can cause a disturbance in the models and understanding the data. These are not data points we are analyzing and are really just indicators of facilites in this case and thus there isn't anything to analyze 

## 3.2 Assess missingness & devise an imputation plan
```{r}
# Drop ID and State and analyze the missingness in pairs using the mice package
p <- md.pairs(HEART_FAILURE_AnalyzeDemo[,-c(1:2)])
## Calculate pairwise pattern of missingness, where value is missing in both columns being compared.
df <- as.matrix(p$mr/(p$mr+p$mm))
```

```{r, echo = FALSE, fig.width = 7, fig.height = 7}
## Set a color palette
prettyPurples <- colorRampPalette(brewer.pal(8, "PuRd"))(8)
## Make a heatmap
heatmap(df, 
        col = prettyPurples, 
        cexRow = 0.5, 
        cexCol = 0.5, 
        margins = c(13, 13), 
        Colv = NA)
```

##### **Question 7**: [1 point]
What columns seem to be most correlated in terms of missingness with our target, `PredictedReadmissionRate`? Does this give you any cause for concern?

> #Malek
Interestingly enough, the heat map is showing the same missingness columns which are SurveyResponseRate, healthcare workers given influenza vaccination, and median time patients spent in ED

### Drop the rows that are missing from the target variable. 


```{r, echo = FALSE}
HEART_FAILURE_AnalyzeDemo <- HEART_FAILURE_AnalyzeDemo %>% 
  filter(!is.na(ExpectedReadmissionRate)) 

paste0(c("Rows: ", "Columns: "), dim(HEART_FAILURE_AnalyzeDemo))
```

## 3.3 Split into training & testing sets.

##### **Question 8**: [2 points]
There are three parts to this question to get it to all come together correctly. **Practice reading R code**
#Malek
* Comment the function to explain what it does (I've defined the arguments for you)
* Run the function (fill in the blank)
* Fill in the missing piece in the partitioning chunk below

```{r}
#Malek
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ##With no p, it is set to the number of columns in the dataframe - 1
  if(is.na(p)) {
    p <- ncol(df) -1   ##parameters is columns - 1
  }
  
  ##So now we are builiding the test set size and using that important formula of (1/sqrt(p))
  test_N <- (1/sqrt(p))*nrow(HEART_FAILURE_AnalyzeDemo)
  ##Determines the proportion of test set compared to the whole data frame
  test_prop <- round((1/sqrt(p))*nrow(HEART_FAILURE_AnalyzeDemo)/nrow(HEART_FAILURE_AnalyzeDemo), 2)
  ##Training proportion, whatever the test isn't
  train_prop <- 1-test_prop
  
  ##Prints best ratio
  print(paste0("The ideal split ratio is ", train_prop, " : ", test_prop, " training : testing"))
  
  ##Returns training proportion
  return(train_prop)
}

## Fill in the blanks to run:
train_prop <- calcSplitRatio(df = HEART_FAILURE_AnalyzeDemo)
```

**Hint**: One important note here is that if the number of parameters are not provided (which is true for the starter code I gave you to run it - it does NOT pass any parameters to that argument!), then by default our function is designed to take the number of columns of the dataframe and subtract one for the target variable. Neat, huh?

You should have gotten an __83-17 split__. Notice how that's actually very close to the canonical 80-20 split!

```{r}
#Malek
ind <- createDataPartition(HEART_FAILURE_AnalyzeDemo$ExpectedReadmissionRate, 
                            p = train_prop,
                            list = FALSE)

HFtrain <- HEART_FAILURE_AnalyzeDemo[ind, ]
HFtest <- HEART_FAILURE_AnalyzeDemo[-c(ind), ]
```


## 3.4 Impute missing variables using `missForest`.
```{r, include = FALSE}
# Remove encoded categorical variables for imputation, plus the target and state (which have no missing)
data_sans_encoded = HFtrain %>% 
  select(-ExpectedReadmissionRate,
         -State,
         -`Emergency department volume`, 
         -contains("ComparedToNational"))

data_cat = HFtrain %>% 
  select(`Emergency department volume`, 
         contains("ComparedToNational")) %>% 
  mutate_all(as.factor)

# Stitch them back together:
temp <- cbind(data_sans_encoded, data_cat)

# Impute missing values using missForest
HFimputedTrain <- missForest(temp, 
                          variablewise = TRUE,
                          verbose = FALSE)
```

Let's look at a quick summary of the results.

```{r, echo = FALSE}
# Show a quick summary of the results
data.frame(HFimputedTrain$OOBerror, names(temp)) %>% 
  rename(Variable = `names.temp.`,
         `OOB Error` = HFimputedTrain.OOBerror) %>% 
## Print the columns
  kable(digits = 2,
    format = "html",
    caption = "Table 4. missForest OOB Error Rates for the imputed variables, training dataset") %>%
    pack_rows("MSE", 1, 26) %>%
    pack_rows("PCF", 27, 31) %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

##### **Question 9**: [1 point]
What does `MSE` and `PCF` indicate here? You may find looking at the `missForest` [Vignette](https://stat.ethz.ch/education/semesters/ss2012/ams/paper/missForest_1.2.pdf) helpful. How is the random forest that was performed here similar to what we performed on the gene expression data? How does this differ?

>#Malek
This question was kind of confusing for me, but MSE I believe is the mean squared error. What is being measured in the mean squared error is the average of all the differences between the true and the estimated values. PCF, partial complete fraction looks at the proportion of correctly observed values. The thing that I want to point out here is that in gene expression the forests are applied based on gene expression patterns, but on the flip side missforest looks at missing values and builds forests from there I believe. 

```{r}
HFimputedTrain <- HFimputedTrain$ximp %>% 
  ## Put the original State and target variables back on
  mutate(State = HFtrain$State,
         ExpectedctedReadmissionRate = HFtrain$ExpectedctedReadmissionRate) %>% 
  ## Convert the factors back to character type...
  mutate_at(c("Emergency department volume",
              "ComparedToNational_CMS Medicare PSI 90: Patient safety and adverse events composite",
              "ComparedToNational_MRSA Bacteremia",
              "ComparedToNational_Death rate for heart failure patients",
              "ComparedToNational_Hospital return days for heart failure patients"),
            as.character) %>% 
  ## ... so we can then convert them back to numeric type.
  mutate_at(c("Emergency department volume",
              "ComparedToNational_CMS Medicare PSI 90: Patient safety and adverse events composite",
              "ComparedToNational_MRSA Bacteremia",
              "ComparedToNational_Death rate for heart failure patients",
              "ComparedToNational_Hospital return days for heart failure patients"),
            as.numeric)


HFimputedTrain <- cbind(HFimputedTrain, HFtrain$ExpectedReadmissionRate)
names(HFimputedTrain)[names(HFimputedTrain) == "HFtrain$ExpectedReadmissionRate"] <- "ExpectedReadmissionRate"


## Save data files so they can be loaded for future use 
# save(imputedTrain, file = "imputedTrain.Rdata")
```


##### **Question 10**: [1 point]
Now repeat the imputation steps for the `test` dataset, storing it into `imputedTest`.
```{r}
## Ryan C

# Remove encoded categorical variables for imputation, plus the target and state (which have no missing)
data_sans_encoded2 = HFtest %>% 
  select(-ExpectedReadmissionRate,
         -State,
         -`Emergency department volume`, 
         -contains("ComparedToNational"))

data_cat2 = HFtest %>% 
  select(`Emergency department volume`, 
         contains("ComparedToNational")) %>% 
  mutate_all(as.factor)

# Stitch them back together:
temptest <- cbind(data_sans_encoded2, data_cat2)

# Impute missing values using missForest
HFimputedTest <- missForest(temptest, 
                          variablewise = TRUE,
                          verbose = FALSE)

## Malek
HFimputedTest <- HFimputedTest$ximp %>% 
  ## Put the original State and target variables back on
  mutate(State = HFtest$State,
         ExpectedReadmissionRate = HFtest$ExpectedReadmissionRate) %>% 
  ## Convert the factors back to character type...
  mutate_at(c("Emergency department volume",
              "ComparedToNational_CMS Medicare PSI 90: Patient safety and adverse events composite",
              "ComparedToNational_MRSA Bacteremia",
              "ComparedToNational_Death rate for heart failure patients",
              "ComparedToNational_Hospital return days for heart failure patients"),
            as.character) %>% 
  mutate_at(c("Emergency department volume",
              "ComparedToNational_CMS Medicare PSI 90: Patient safety and adverse events composite",
              "ComparedToNational_MRSA Bacteremia",
              "ComparedToNational_Death rate for heart failure patients",
              "ComparedToNational_Hospital return days for heart failure patients"),
            as.numeric)


## Save data files to be used in future
# save(imputedTest, file = "imputedTest.Rdata")
```


##### **Question 11**: [1 point]
Quickly explore how well the imputation did by choosing at least one numeric variable and one of the categorical variables. Did the distributions or frequencies change drastically?

```{r}
# Your code here.
```

> Your answer here.
         
Struggling? Here's my solution:
```{r, warning = FALSE, message=FALSE}
# source(file = "answer_question11.R", echo = TRUE)
```


## 3.5  Transformation & Scaling

**In other words: is our target variable approximately normally distributed?**

```{r, echo = FALSE, warning=FALSE, message=FALSE}
ggplot(HFimputedTrain, aes(x = ExpectedReadmissionRate)) +
  geom_histogram(alpha = 0.6, 
                 fill = prettyPurples[5],
                 color = prettyPurples[2]) +
  theme_minimal() +
  labs(title = "Predicted Readmission Ratio, N = 2,726 hospitals",
       y = "Frequency",
       x = "Predicted Readmission Ratio")
```

##### **Question 12**: [1 point]
Why is a histogram, or even a Q-Q plot, an insufficient way to assess whether a distribution is approximately normal?

> Your answer here.while histograms and Q-Q plots are valuable tools, their limitations necessitate the use of complementary statistical tests to more accurately assess whether a distribution is approximately normal.


##### **Question 13**: [1 point]
Apply a Shapiro-Wilk test for normality using the `shapiro.test()` function. What does it indicate about your distribution? (**Note**: Shapiro tests are only reliable for $N < 5000$, but it is fine to perform here.)

```{r}
shapiro_test_result <- shapiro.test(HFimputedTrain$ExpectedReadmissionRate)

# Printing the result
print(shapiro_test_result)
```
Shapiro-Wilk normality test

data:  HFimputedTrain$ExpectedReadmissionRate
W = 0.99643, p-value = 0.00003853
> Your answer here.Distribution is normalized 

### 3.5.1 Box-Cox Transformation

```{r}
bc_data <- BoxCoxTrans(HFimputedTrain$ExpectedReadmissionRate)
bc_data
```

So, we can see here that the estimated lambda is -0.3; so not quite a full square root transform.

Now apply the Box-Cox transformation:
```{r, echo = TRUE}
HFimputedTrain$bc_ExpectedReadmissionRate <- predict(bc_data, HFimputedTrain$ExpectedReadmissionRate)
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
ggplot(HFimputedTrain, aes(x = bc_ExpectedReadmissionRate)) +
  geom_histogram(alpha = 0.6, 
                 fill = prettyPurples[7],
                 color = prettyPurples[2]) +
  theme_minimal() +
  labs(title = "Box-Cox Transformed Predicted Readmission Ratio, N = 2,726 hospitals",
       y = "Frequency",
       x = "Expected Readmission Ratio")


qqnorm(HFimputedTrain$bc_ExpectedReadmissionRate)
qqline(HFimputedTrain$bc_ExpectedReadmissionRate, 
       col = "hotpink", lty = 2, lwd = 2)
```

##### **Question 14**: [1 point]
Now repeat all of the steps to perform a Box-Cox transform on your own to the `imputedTest` data. Why do you think we need to transform the test data too?
> Tiffany
> Your answer here.If train data is transformed, we must transform test data as well to get more acurate results

```{r}
## Ryan C
bc_data2 <- BoxCoxTrans(HFimputedTest$ExpectedReadmissionRate)
HFimputedTest$bc_ExpectedReadmissionRate <- predict(bc_data2, HFimputedTest$ExpectedReadmissionRate)

```

### 3.5.2 Centering & Scaling

##### **Question 15**: [1 point]
## Ryan C
Make a copy of the imputed training set, drop our original, non-Box-Cox transformed target from the dataset, and then center and scale the whole training set using the `scale()` function in R. I called my dataset `readyTrain`, so if you name it anything else just note you'll have to update subsequent code. Then, __do the same thing for the test data__!

```{r}
center_and_scale <- function(data) {
  scaled_data <- scale(data)
  return(scaled_data)
}

HFimputedTrain <- HFimputedTrain %>% select(-ExpectedReadmissionRate)
HFimputedTest <- HFimputedTest %>% select(-ExpectedReadmissionRate)

HFreadyTrain <- center_and_scale(HFimputedTrain)
HFreadyTest <- center_and_scale(HFimputedTest)

HFreadyTrain <- HFreadyTrain %>% data.frame()
HFreadyTest <- HFreadyTest %>% data.frame()


```

# 4 Unsupervised Learning Methods

## 4.1 Principal Component Analysis

```{r}
pca <- prcomp(HFreadyTrain)
summary(pca)
```

Let's also look at this as a Scree Plot.
```{r, echo= FALSE}
## Show the scree plot
fviz_screeplot(pca, 
         addlabels = TRUE, 
         ylim = c(0, 30),
         ncp = 15,
         barfill = prettyPurples[7],
         barcolor = prettyPurples[3],
         main = "Scree Plot: First 15 Principal Components")
```

##### **Question 16**: [1 point]
Using the 'elbow' method of the scree plot - the point at which the percentage of variance explained seems to level off - approximately how many principal components would be significant to explain the total variation in the training data set? (**Hint**: if it's not super obvious, that's actually informative!)

> Your answer here

```{r, echo = FALSE}
## Create a factor variable that contains the information about the compared to national average - return to hospital days variable
returnHospitalComparison <- factor(HFreadyTrain$ComparedToNational_Hospital.return.days.for.heart.failure.patients)
levels(returnHospitalComparison) <- c("Worse", "Same", "Better")

fviz_pca_biplot(pca, 
             palette = c("maroon", "cadetblue", "gold"),
             label = "none",
             geom = "point",
             geom.var = "text",
             addEllipses = TRUE,
             col.var = "black",
             habillage = returnHospitalComparison,
             select.var = list(contrib = 5))
```

##### **Question 17**: [1 point]
On principal components 1 and 2 (the PCs that account for the __most__ variance in the training data), do you see any pattern (i.e., clustering) relative to the hospital return days for pneumonia patients compared to the national average? Why or why not? 

> Your answer here

```{r, echo=FALSE, fig.width=5, fig.height=5}
fviz_contrib(pca, 
             choice = "var", 
             axes = 1:2,
             fill = prettyPurples[3],
             color = prettyPurples[4],
             xtickslab.rt = 70)
```

## 4.2 *k*-means Clustering

```{r, echo = F, warning = FALSE, message = FALSE}
myPal <- c("#de378d","#32a840", "#463ab5","#0e87e3", "gold", "#ae5bc9", "#e35c0e", "cadetblue", "#0ee383")
for(k in 2:9) {
  plotName <- paste0("p", k)
  kmeansResult <- kmeans(HFreadyTrain, 	
                         centers = k,       ## number of clusters
                         nstart = 25,       ## num of times to repeat the process with random initialization; increase if you're failing to converge
                         iter.max = 1000,    ## num of iterations to run k-means
                      	algorithm = "MacQueen")      ## since the default algorithm can struggle with close points, we are adjusting the method
  kmeansGraph <- fviz_cluster(kmeansResult, 
                              geom = "point", 
                              data = HFreadyTrain,
                              show.clust.cent = TRUE,
                              ggtheme = theme_minimal(),
                              ellipse.type = "norm",
                              palette = myPal,
                              pointsize = 0.5) + 
                  ggtitle(paste0("k = ", k))
  assign(plotName, kmeansGraph)
}
grid.arrange(p2, p3, p4, p5, nrow = 2)
grid.arrange(p6, p7, p8, p9, nrow = 2)
```

##### **Question 18**: [1 point]
Which number of clusters, $k$, do you think has the best explanatory power? Is it hard to tell?

> Your answer here

```{r, echo = FALSE}
# Determine number of clusters
fviz_nbclust(HFreadyTrain,
             FUNcluster = kmeans, 
             method = "wss",  
             linecolor = prettyPurples[7])
```

#### **Question 19**: [1 point]
What is the __within sum of squares__ and what exactly is it measuring here? By the elbow method, which $k$ is the optimal number of clusters? Did it agree with your choice from the other heuristic method?

> Your answer here


## 4.3 Segmentation Analysis

#### **Question 20**: [1 point]
Let's explore the clusters - segments - of the hospitals based on their average values from the __original__ dataset. Your task is to add comments to this code chunk. 

```{r, echo = TRUE, fig.height = 6, fig.width=6}
## Uses the KMeans function on the readyTrain dataset and stores it in kmeansResult. Below are the parameters.
HFkmeansResult <- kmeans(HFreadyTrain, 	
                         centers = 3,            ## Specifies 3 clusters for k-means clustering.
                         nstart = 50,            ## Number of initial sets of cluster centers to try.
                         iter.max = 1000,        ## Maximum number of iterations.
                         algorithm = "MacQueen") ## Algorithm used for clustering.

## Adds a column called Cluster based on result.
HFkmeansTrain <- HFimputedTrain %>% 
  mutate(Cluster = HFkmeansResult$cluster) %>% 
  clean_names()
```

```{r}
## Specifies which variables to import for PCA.
pcaImportantVars <- c("overall_hospital_rating",
                      "care_transition",
                      "nurse_communication",
                      "recommend_hospital",
                      "staff_responsiveness",
                      "communication_about_medicines",
                      "doctor_communication",
                      "discharge_information",
                      "death_rate_for_heart_failure_patients",
                      "quietness",
                      "average_median_time_patients_spent_in_the_emergency_department_before_leaving_from_the_visit_a_lower_number_of_minutes_is_better",
                      "hospital_return_days_for_heart_failure_patients",
                      "emergency_department_volume", 
                      "compared_to_national_hospital_return_days_for_heart_failure_patients",
                      "cleanliness",
                      "bc_expected_readmission_rate")


HFkmeansTrain %>% 
  select(all_of(pcaImportantVars), cluster) %>%
  rename("Hospital Return Days Score" = hospital_return_days_for_heart_failure_patients,
         "Hospital Return Days Nat'l Comparison" = compared_to_national_hospital_return_days_for_heart_failure_patients,
         "Median Minutes spent in Emergency Dept" = average_median_time_patients_spent_in_the_emergency_department_before_leaving_from_the_visit_a_lower_number_of_minutes_is_better) %>%
  clean_names(case = "title") %>%
  group_by(Cluster) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  pivot_longer(-1, names_to = "Measure", values_to = "Value") %>%
  ggplot(aes(x = Cluster, y = Value, fill = as.factor(Cluster))) +
    geom_col(alpha = 0.75) +
    scale_fill_manual(values = myPal) +
    labs(title = "Cluster Means",
         x = "",
         fill = "Cluster") +
    theme_minimal() + 
    theme(legend.position = "bottom",
          strip.text = element_text(size=6),
          legend.text = element_text(size = 8),
          legend.title = element_text(size = 10),
          title = element_text(size = 12, color = "maroon")) + 
    facet_wrap(~Measure, 
               scales = "free_y",
               ncol = 3)
``` 


#### **Question 21**: [1 point]
## Ryan C
Notice, for example, that Cluster 2 hospitals seem to include the hospitals that had the __below__ national average return hospital days (and thus the fewest return hospital days in terms of the quantitative score). Look at how else you might broadly characterize - or segment - these hospitals.

Your job is to make a table for our client, summarizing each of the four clusters. Give them an informative Title - e.g., "Highest Performing Hospitals" for cluster 1, perhaps? 

Then come up with at least FOUR characteristics per cluster. You may choose to focus on different attributes for different clusters. Fill in the code for the table below. I start you out with two examples for Cluster 1, but feel free to add more! You are also welcome to rename Cluster 1 if you think of a catchier name than I chose!

```{r, echo = FALSE, collapse=TRUE}
library(tibble)
library(kableExtra)

dict <- tribble(
~`Top Defining Attributes`, ~`Description`,
"Doctor Communication", "Best doctor communication compared to other clusters",
"Nurse Communication", "Highest scored out of all the clusters",
"Staff Responsiveness", "Highest scores across all communication aspects",
"Highest Communication about Medicines", "Well above average for commuicating about medicines",


"Overall Hospital Rating", "Highest rated Hospitals", 
"Care Transition", "Best at transitioning paitents",
"Cleanliness", "Cleanest hospitals",
"Quietness", "Quietest opitals",

"Death Rate for HF Paitents", "Lowest death rate when compared to other clusters",
"Recommended Hospital", "Highest Recommended Hospital",
"Emergency Department Volume", "Large amounts of volume",
"Time spent in Emergency Room", "Reported to have the lowest median time in the ED",
)


dict %>% 
  kable(
    format = "html",
    caption = "Table 5. Hospital segmentation analysis: three types of broad groupings identified.") %>%
    pack_rows("1: Best Hospital for Communicating information ", 1, 4) %>%
    pack_rows("2: Highest Performing Hospitals", 5, 8) %>%
    pack_rows("3: Best Ranked Emergency Department", 9, 12) %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

## 4.4 Final Remarks on Segmentation

#### **Question 22**: [1 point]
## Ryan C
Make a recommendation for our client for 1-2 machine learning analyses __your team__ would choose to use to test whether these clusters can be used for robust predictions of hospital performance for pneumonia patients. How would you know whether or not your predictions were robust? What would you look for or compare?

> I would recommend a DBSCAN and logistic regression to test and see if the clusters can predict heart failure outcomes accurately based on the given historical data. Then, I'd preform a decision tree analysis to try and understand the factors that influence the predictions made by those clusters. To check if predictions are robust, I'd just compare the predicted outcomes with actual outcomes from a new set of pneumonia patient data, and check different metricslike accuracy, sensitivity, and specificity. If the model consistently predicts outcomes across different datasets, it indicates robustness.

# 4 Supervised Learning Methods

#### **Question 23**: [1 point]
## Ryan C
Could our PCA benefit from re-running it after dealing with possible multicollinearity? Explain your reasoning.

> Yes, re-running PCA after addressing any possible multicollinearity problems woudl benefit the analysis. By addressing any multicollinearity, it would increase the variance of the principal components. This would hopefully make stronger components that wouldnt be dominated by only a few variables.

## 4.1 Multicollinearity Assessment

## 4.1 Pairwise Correlation 

```{r, echo = FALSE, fig.width = 7, fig.height = 7}
corrMat2 <- cor(HFreadyTrain)

## Make a heatmap
heatmap(corrMat2, 
        col = rev(colorRampPalette(brewer.pal(8, "RdYlBu"))(8)), 
        cexRow = 0.5, 
        cexCol = 0.5, 
        margins = c(10, 10))
```

##### **Question 24**: [1 point]
## Ryan C
Where do you see the highest potential for multicollinearity? Why do you say that?

> The highest potential for multicollinearity would be in the top right of the heat map which would be all the variables about communication, ratings, information, and the state of the hospital (cleanliness, quietness).


## 4.2 OLS Multiple Linear Regression 

### 4.2.1 Fitting a model to estimate VIF

```{r, echo=FALSE, fig.height=6, fig.width=6}
mod <- lm(bc_ExpectedReadmissionRate ~ ., data = HFreadyTrain)
par(mfrow=c(2,2))
plot(mod, 1)
plot(mod, 2)
plot(mod, 3)
plot(mod, 4)
```

##### **Question 25**: [1 point]
## Ryan C
You may have to do a little outside research, but interpret the four plots that have come off the regression. Do we largely meet the assumptions for an OLS multiple regression? Why or why not? 

> Yes all four plots largely met the assumptions for an OLS multiple regression. The residuals vs. fitted and scale-loaction plots data both appear to be randomly scattered on the plot across the red line.all the points on the cooks distance plot are close to 0 and even the outliers are not farter the 0.03 away. Finally the Q-Q residuals plot shows that most of the points fall along the line which is what we expexct.

```{r}
shapiro.test(residuals(mod))
```

### 4.2.2 Estimating the VIF (Variance Inflation Factors)

```{r, echo = FALSE, warning=FALSE,message=FALSE}
## Calculate the VIFs and put into a dataframe to print the table
vifResults <- vif(mod) %>% data.frame
## Change the column names of the dataframe
colnames(vifResults) <- c("VIF")

## Sort by VIF, print the top 15 worst offenders
vifResults %>% 
  arrange(desc(VIF)) %>% 
  top_n(15) %>% 
  ## Pass through kable() to make it pretty
  kable(digits = 2,
    format = "html",
    caption = "Table 6. Variance Inflation Factors after multiple linear regression") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

##### **Question 26**: [1 point]
## Ryan C
Does multicollinearity seem to be an issue in this dataset and, if so, among which variables? Why do you come to this conclusion?

> After reading this article https://corporatefinanceinstitute.com/resources/data-science/variance-inflation-factor-vif/#:~:text=Generally%2C%20a%20VIF%20above%204,that%20needs%20to%20be%20corrected. I found that multiple variables have high multicollinearity mainly all the varaibles above 5 have strong multicollinearity so tese 5 need to be addressed 
Overall.hospital.rating	24.16
Recommend.hospital	16.69
Care.transition	8.76
Nurse.communication	8.57
Staff.responsiveness	5.46

### 4.2.3 Dropping the most collinear variables and re-running the OLS regression.

```{r}
## Drop those with VIF over 4
HFregressionTrain <- HFreadyTrain %>% 
  select(-Overall.hospital.rating, -Recommend.hospital, -Nurse.communication, -Care.transition, -Staff.responsiveness, -Communication.about.medicines, -Doctor.communication)

## Do the same for the testing data!
HFregressionTest <- HFreadyTest %>% 
  select(-Overall.hospital.rating, -Recommend.hospital, -Nurse.communication, -Care.transition, -Staff.responsiveness, -Communication.about.medicines, -Doctor.communication)
```

```{r, echo = FALSE}
## Fit a linear model
mod <- lm(bc_ExpectedReadmissionRate ~ ., data = HFregressionTrain)
## Extract just the R-squared
summary(mod)$adj.r.squared
```

```{r, results='hide'}
## Perform a backward, stepwise regression to find the most parsimonious model
bestMod <- step(mod, 
            direction = "backward", 
            trace = FALSE)
```

```{r, results='asis'}
## Print a table using stargazer
stargazer(bestMod, 
          type = "html",
          title = "Table 7. Parsimonious OLS Regression Results.")
```

```{r}
## Make the predictions
predictions <- predict(bestMod, newdata = HFregressionTest)
## Calculate the RMSE
RMSE(predictions, HFregressionTest$bc_ExpectedReadmissionRate)

maxvalues <- max(HFreadyTrain$bc_ExpectedReadmissionRate)
maxvalues
minvalues <- min(HFreadyTrain$bc_ExpectedReadmissionRate)
minvalues

```

##### **Question 27**: [1 point]
## Ryan C
Can you interpret what the RMSE tells your client here? Is this a good model? An okay model? 

**Hint 1**: Remember that the outcome here is Box-Cox transformed. Would you need to undo the transformation to be able to assess what the RMSE is actually telling us? Recall that the Box-Cox $\hat{y} = \frac{1}{y^\lambda}$ where our $\lambda = -0.3$ and we also centered and scaled these data. 

**Hint 2**: Is it practical to undo both transformations? How does that hinder our ability to make an interpretation for our client?

> It is not practical to directly interpret the RMSE value in its current form since it has been transformed. It might not be practical to undo both transformations because it can be complex and hinder the ability to make a straightforward interpretation for our client. In this case it would be best to compare compare the RMSE of this model to an alternative model or to a baseline model because it would be more simple and strightforward.


```{r, echo = FALSE, warning=FALSE, message=FALSE}
## Add the predictions to the dataset
HFregressionTest$predictions <- predictions

## Graph what the predictions look like relative to the actual data
ggplot(HFregressionTest, 
       aes(x = predictions, y = bc_ExpectedReadmissionRate)) +
       geom_point(alpha = 0.5, color = "hotpink") +
       geom_smooth(color = "hotpink", fill = "hotpink", se = T, method = "lm") +
  theme_minimal() +
  labs(title = "OLS Multiple Linear Regression Predictive Power",
       x = "Predicted Readmission Rate from Model",
       y = "Actual Readmission Rate")
```


### 4.3 The Elastic Net

## 4.3.1 Running, tuning, & cross-validating the Elastic Net
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 5, 
                     search = "grid", 
                     verboseIter = FALSE)
```

```{r}
searchGrid <- expand.grid(.alpha = seq(0, 1, length.out = 10), .lambda = seq(0, 5, length.out = 15))
elasticMod <- train(bc_ExpectedReadmissionRate ~ ., 
                    data = HFreadyTrain[, -26], 
                    method = "glmnet", 
                    tuneGrid = searchGrid,
                    trControl = ctrl) 
```

```{r}
sum(colSums(is.na(HFreadyTrain)))
```

##### **Question 28**: [1 point]
## Ryan C
Explain what exactly we are doing here. Remind yourself of the parameters of the `trainControl()` and `train()` functions, and look up any new ones you do not yet know. Specifically, make sure to address what type of CV and hyperparameter search is being performed, as well as if both the $\alpha$ and $\lambda$ hyperparameters of elastic net are being tuned.

> We are doing a grid search to tune the hyperparameters of the model. We are doing a 10 fold cross validation. We search alpha values from 0 to 1 and lambda values from 0 to 5.

## 4.3.2 Elastic Net Results
We are going to first take a look at the first 15 results of the Elastic Net in a table form.
```{r, echo = FALSE, warning = FALSE, message = FALSE}
elasticMod$results %>% 
  data.frame() %>% 
  top_n(30) %>% 
kable(digits = 2,
    format = "html",
    caption = "Table 7. Results of the Elastic Net Tuning & 10-fold Cross-Validation") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

#### What were the hyperparameters of our best tuned model?

$\alpha$:
```{r, echo=FALSE}
elasticMod$bestTune$alpha
```
$\lambda$:
```{r,echo=FALSE}
elasticMod$bestTune$lambda
```

##### **Question 28**: [1 point]
## Ryan C
Check the values of $\alpha$ and $\lambda$ here. What do they suggest about the optimal regularization that is being fit here? (Recall that when both $\alpha$ and $\lambda$ are zero, it's an OLS regression!)

> The values $\alpha$ and $\lambda$ are 0 and 0 which suggest that the model is still performing OLS regression. This it suggests that the model did not apply any regularization to the coefficients. This means that the model is attempting to find the best fit to the training data without any constraints on the coefficients.

```{r, echo = FALSE, fig.width=4, fig.height=5, warning=FALSE, message=FALSE}
results <- elasticMod$results %>% 
  data.frame()
best <- results[as.numeric(rownames(elasticMod$bestTune)), ]

results %>% 
  drop_na() %>% 
  mutate(lambda = round(lambda, 1)) %>%  
  ggplot(aes(x=alpha, y = RMSE, color = as.factor(lambda))) +
  geom_point() +
  theme_minimal() +
  geom_point(aes(x = best$alpha, best$RMSE), 
             shape = 5, 
             size = 3,
             color = "black") +
  labs(title = "Elastic Net Hyperparameter Tuning",
       subtitle = "Best mixing percentage + Î» shown as a diamond",
       x = expression(alpha), 
       color = expression(lambda)) + 
  scale_color_manual(values = c(myPal, "#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#00AFBB", "#E7B800", "#FC4E07"))
```

#### Important Features
```{r, echo = FALSE, fig.height=5, fig.width=10}
important <- varImp(elasticMod)$importance
important %>% 
  mutate(Feature = rownames(important)) %>% 
  mutate(Feature = gsub("\\.", " ", Feature)) %>% 
  arrange(desc(Overall)) %>% 
  ggplot(aes(y = Overall, fill = Overall, x = fct_reorder(Feature, Overall))) +
  geom_col() + 
  scale_fill_continuous(low = prettyPurples[3], high = prettyPurples[7]) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature importance as determined by Elastic Net",
       x = "",
       y = "Importance", 
       fill = "")
```

##### **Question 29**: [1 point]
## Ryan C
Which of the features were most important? How does this compare to the most important features out of regression? How about about of PCA?

> The most important features wre Recommend hospital, Death Rate for heart failure paitents, Emergency department volume, and median time in ED. This is different when comparing the VIF and PCA dimensions which are different and focus more on communication.

```{r}
prOLS <- postResample(pred = predict(mod, newdata = HFregressionTest), 
                      obs = HFregressionTest$bc_ExpectedReadmissionRate)

prElastic <- postResample(pred = predict(elasticMod, newdata = HFreadyTest), 
                      obs = HFreadyTest$bc_ExpectedReadmissionRate)

## Display the output
rbind(prOLS, prElastic) %>% 
  kable(digits = 2,
    format = "html",
    caption = "Table 8. Comparison of the OLS and Elastic Net Regressions") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

##### **Question 30**: [1 point]
## Ryan C
Using the same method, check for overfitting / underfitting. Make predictions using `readyTrain` rather than `readyTest`, and then check how the RMSE on the training data compares to the RMSE of the training data for the Elastic Net. How do they compare? (Recall what we discussed when we did the gene expression demo: the relationship of the test vs. training accuracy can be used to assess overfitting/underfitting. The same logic applies to RMSE too!) __How do both compare to what we got out of the OLS regression?__ Which would you say is the better predictive model, the OLS regression or the Elastic Net? Why?

>  After comparing all the numbers in the table below we can see that they are all the exact same between the two models. When comparing the RMSE the R^2 and MAE for the training and testing sets we can see that OLS regression and the Elastic net are the same but both are starting to slightly overfit. These two models are the same becasue the best alpha and lambda values for the Eleastic Net are both 0 making it an OLS regression. Below is the breakdown for the metrics

> For both the OLS regression and Eleastic net the RMSE is lower on training data compared to testing data, R^2 is higher on training data compared to testing data, and the MAE is lower on training data compared to testing data. All of these point towards the model starting to overfit.

> Even thought the model is slightly overfitting the I think the OLS regression preforms slightly better because the values of the RMSE, R-squared, and MAE are closer then the values for the Elastic Net model. Becasue of this when given more unseen data be stronger at making predictions. Even though this is close to what we want, it doesn't necessarily mean that the model is optimal. We should still consider other evaluation metrics and perform more cross-validation to ensure the reliability and robustness of the model.

```{r}
prOLS_train <- postResample(pred = predict(mod, newdata = HFreadyTrain), 
                            obs = HFreadyTrain$bc_ExpectedReadmissionRate)
prElastic_train <- postResample(pred = predict(elasticMod, newdata = HFreadyTrain), 
                                obs = HFreadyTrain$bc_ExpectedReadmissionRate)

rbind(prOLS_train, prOLS, prElastic_train, prElastic) %>%
  kable(digits = 2,
        format = "html",
        caption = "Table 8. Comparison of the OLS and Elastic Net Regressions on Training Data") %>%
  kable_styling(bootstrap_options = c("hover", full_width = F))

```

A graphical comparison of the two models.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
elasticPredictions <- predict(elasticMod, newdata = HFreadyTest)
testingResults <- c(HFregressionTest$predictions,
                    elasticPredictions)
label <- c(rep("OLS Prediction", nrow(HFregressionTest)),
           rep("Elastic Net Prediction", length(elasticPredictions)))
temp <- cbind.data.frame(testingResults, label)
temp$actual <- rep(HFregressionTest$bc_ExpectedReadmissionRate, 2)

## Graph what the predictions look like relative to the actual data
ggplot(temp, 
       aes(x = testingResults, y = actual, fill = label, color = label)) +
       geom_point(alpha = 0.5) +
       geom_smooth(se = T, method = "lm", alpha = 0.45) +
  scale_color_manual(values = c("navy", "hotpink")) +
  scale_fill_manual(values = c("navy", "hotpink")) +
  theme_minimal() +
  labs(title = "Elastic Net vs. OLS Predictive Power",
       x = "Predicted Readmission Rate from Model",
       y = "Actual Readmission Rate",
       fill = "", 
       color = "")
```

Unsurprisingly from the RMSE, these models are performing VERY similarly. 

##### **Question 31**: [4 points]
## Ryan C
What conclusions do you make at this point about whether to make predictions with an OLS regression, Elastic Net, or neither? What recommendations would you make to our client at this point? What other analyses might your suggest, or other data we might want to include? Please try to flesh out 2-3 recommendations in at least a paragraph.

At this point, the decision to use predictions from an OLS regression depends on the specific objectives and constraints of the project. Here are some conclusions and recommendations based on the analysis:

> To conclude, both the OLS regression model and Eleastic net model are showing signs slight of overfitting which can be seen in the table above. In this table the metrics are lower on the training data compared to the testing data. However, it still generally outperforms the Elastic Net model even though they are close in terms of predictive accuracy on unseen data. Despite the overfitting, the OLS regression model can still provide valuable insights and predictions on paitents with pneumonia-related illnesses.

> To the clients, I would recommend another regularization technique like Lasso to try to fix the overfitting problem. By fixing this, this would also improve the performance of the model. 
> We could also explore other optons like trying different ensemble methods like boosting and bagging. The aim of using these methods are to improve prediction accuracy by combining multiple models, reducing overfitting, and offer an easy way to get insights into data by looking at eac individual model contributions.
> Another recommendation I would give is to look at how the data was collected so we can improve the model by giving it different or better data. We can update the datasets to add any new or relevant data. We can look at domain-specific variables to see if demographics, medical history, and treatment protocols could provide valuable insights into readmission rates.

>Finally we can check back in with the paitents at a later time to see if any new patterns emerge that were not there intially. Even though the OLS regression model still has some limitations, it still provideding valuable predictions and insights. If we employ the recommendations above I am confident that the models prediction accuracy will further improve and do a better job at meeting our objectives.

#### Adventure 1. [Choosing to analyze HF.]
* Make sure that you use the code I provided, updating it for your new condition
* Make sure to answer all of the interpretative questions again for your new dataset.
* I will specifically be looking for a comparison of which condition, pneumonia or the one you chose, seems to be a better choice for a predictive model for our client. Or is it neither?
* Make sure to include some next steps or recommendations based on your new analysis!



## 5.2 Deliverables

Unlike what you may have seen on Canvas, I am going to make things much lighter on you this time so we can all catch a breath. I am looking for 1-2 markdown documents with their knitted HTML. For example, if you're choosing Adventures 1 or 2, you may want to work through this document once, make a copy, and then do it again for the new condition(s) you choose to work through. If you're choosing Adventure 3, maybe you just choose to add on to the bottom of this document, replacing the 'Next Steps' section you see here.

Just make sure to answer the questions well, and make sure to justify the decisions you make. Tell me WHY you're choosing the condition(s) you are in Adventures 1 or 2. Tell me WHY you're doing the analyses you are in Adventure 3. Other than that, make this your own exploratory learning adventure!

